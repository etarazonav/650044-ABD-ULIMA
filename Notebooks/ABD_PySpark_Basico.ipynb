{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding: 0px 10px 0px 0px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Universidad_de_Lima_logo.png/220px-Universidad_de_Lima_logo.png\"  width=\"120\" /> Tutorial de PySpark\n",
    "\n",
    "**Profesor:** Enver G. Tarazona Vargas <br>\n",
    "**Curso:** Analítica con Big Data <br>\n",
    "**FACULTAD DE INGENIERÍA - CARRERA DE INGENIERÍA DE SISTEMAS**<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHAE6TCZTNjQ",
    "outputId": "62df18cb-7ed2-4737-9896-1d419d1c65bf"
   },
   "outputs": [],
   "source": [
    "# Instalar PySpark\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "9_Y_nK7VUYae",
    "outputId": "8a52fad5-1b77-43f0-f316-774e2ab46687"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Inicializar el contexto\n",
    "#spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#sc = spark.sparkContext\n",
    "#sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R7ZjCYiU6O-"
   },
   "source": [
    "# Tema 1: Creación de RDD y Aplicación de `map`\n",
    "\n",
    "En Apache Spark, el Resilient Distributed Dataset (RDD) es una estructura fundamental. Los RDDs permiten que los datos se procesen en paralelo a través de múltiples nodos en un clúster, proporcionando tolerancia a fallos y eficiencia en el manejo de grandes volúmenes de datos. La creación de un RDD se puede realizar de varias formas, incluyendo la distribución de una colección existente o la lectura desde un sistema de almacenamiento externo.\n",
    "\n",
    "Una vez que el RDD está creado, Spark ofrece varias transformaciones para manipular los datos. Una de las transformaciones más básicas y poderosas es `map`, que aplica una función a cada elemento del RDD, transformando los elementos de acuerdo con la función definida. El resultado es un nuevo RDD donde cada elemento es el resultado de aplicar la función al elemento original.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUJYAv0cV8sM"
   },
   "source": [
    "## Ejemplo\n",
    "Para ilustrar cómo trabajar con RDDs y la transformación `map`, consideremos el siguiente ejemplo donde queremos calcular el cuadrado de una serie de números. Este proceso demuestra la capacidad de Spark para aplicar funciones de manera distribuida a través de los datos almacenados en un RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X69XM9mEUrRZ",
    "outputId": "053f2380-39ed-4479-e6a1-910d20e1cf0b"
   },
   "outputs": [],
   "source": [
    "# Creación de un RDD\n",
    "numeros = [1, 2, 3, 4, 5]\n",
    "numeros_rdd = sc.parallelize(numeros)\n",
    "\n",
    "# Aplicación de la función map para calcular el cuadrado de cada número\n",
    "cuadrados_rdd = numeros_rdd.map(lambda x: x**2)\n",
    "\n",
    "# Recoger y mostrar los resultados\n",
    "resultados = cuadrados_rdd.collect()\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KImU-pKoW6kF"
   },
   "source": [
    "## Ejercicios\n",
    "**Ejercicio 1**: Ahora que has visto cómo trabajar con RDDs y la transformación `map`, intenta el siguiente desafío:\n",
    "\n",
    "Dada una lista de precios de productos (por ejemplo, `[4.50, 2.75, 3.35, 5.80]`), crea un RDD y utiliza la función `map` para aplicar un aumento del 10% a cada precio. Recoge y muestra los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcJY7l0VXd0F"
   },
   "source": [
    "**Ejercicio 2**:\n",
    "La gerencia de una cadena de supermercados ha solicitado a un analiista de datos un informe detallado sobre las ventas mensuales de varios productos para entender cuáles merecen una promoción. El objetivo es calcular el total de ventas por producto y aplicar un descuento del 5% a aquellos productos cuyas ventas superen los $100 en el mes, como parte de una estrategia para incentivar mayores ventas.\n",
    "\n",
    "Para ello, en el último mes, se recopiló la siguiente información sobre las ventas de productos:\n",
    "\n",
    "- Manzana: 150 unidades a $0.75 cada una.\n",
    "\n",
    "- Banana: 300 unidades a $0.30 cada una.\n",
    "\n",
    "- Naranja: 200 unidades a $0.50 cada una.\n",
    "\n",
    "- Pera: 50 unidades a $1.25 cada una.\n",
    "\n",
    "- Melón: 80 unidades a $3.50 cada una.\n",
    "\n",
    "A partir de estos datos, en dónde cada registro de venta está representado como una tupla que contiene el nombre del producto, la cantidad vendida y el precio por unidad, realiza las siguientes actividades:\n",
    "\n",
    "1. Construye un RDD con los datos de ventas que has preparado.\n",
    "2. Utiliza la transformación `map` para calcular el total de ventas por producto, multiplicando la cantidad vendida por el precio por unidad.\n",
    "3. Para los productos cuyo total de ventas exceda los $100, aplica un descuento del 5%.\n",
    "4. Genera un informe final mostrando el nombre del producto, las ventas totales originales y las ventas totales después del descuento, si aplicó.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOT9UU03dgiZ"
   },
   "source": [
    "# Tema 2: Uso de `flatMap` y `filter`\n",
    "En PySpark, las transformaciones `flatMap` y `filter` son herramientas poderosas para manipular RDDs. Mientras `map` aplica una función a cada elemento del RDD y mantiene la misma cantidad de elementos de salida como de entrada, `flatMap` puede aumentar o disminuir el número de elementos, ya que \"aplana\" los resultados en una lista. Esto es especialmente útil cuando cada elemento de entrada puede ser mapeado a múltiples elementos de salida. Por otro lado, `filter` permite seleccionar elementos que cumplen con una condición específica, lo cual es útil para depurar datasets o para extracciones condicionales de datos.\n",
    "\n",
    "## Ejemplo\n",
    "Supongamos que necesitamos procesar un conjunto de datos que contiene frases y queremos obtener todas las palabras individuales que contienen más de tres letras. Este proceso será demostrado en el siguiente código, donde utilizaremos `flatMap` para descomponer las frases en palabras individuales y `filter` para seleccionar solo aquellas palabras que cumplen con el criterio especificado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZGVdZOToaut7",
    "outputId": "92363627-729d-4d53-ee9a-587ce9e3398c"
   },
   "outputs": [],
   "source": [
    "# RDD de frases\n",
    "frases = [\"Hola mundo\", \"Aprendiendo PySpark\", \"Hola a todos\"]\n",
    "frases_rdd = sc.parallelize(frases)\n",
    "\n",
    "# Uso de flatMap para descomponer las frases en palabras\n",
    "palabras = frases_rdd.flatMap(lambda frase: frase.split())\n",
    "\n",
    "# Filtrar palabras que tienen más de tres letras\n",
    "palabras_largas = palabras.filter(lambda palabra: len(palabra) > 3)\n",
    "\n",
    "# Recoger y mostrar los resultados\n",
    "resultados = palabras_largas.collect()\n",
    "print(resultados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xba0ebE2e9Cf"
   },
   "source": [
    "**Ejercicio 1**: Un equipo editorial está revisando varios artículos para identificar las palabras clave más frecuentes y relevantes, pero quieren evitar palabras comunes y cortas que no aportan significado. Se te ha proporcionado un conjunto de datos que contiene múltiples frases de artículos. Tu tarea es extraer y listar las palabras que tienen cuatro letras o más, ya que el equipo considera que estas palabras podrían ser más relevantes para sus análisis.\n",
    "\n",
    "Se han extraído varias frases de los artículos, como se muestra a continuación:\n",
    "- \"Innovación y tecnología en el mercado global\"\n",
    "- \"Explorando las profundidades del análisis de datos\"\n",
    "- \"Introducción a la programación en Python para ciencia de datos\"\n",
    "- \"Los desafíos actuales en la inteligencia artificial\"\n",
    "\n",
    "Representa estas frases en una lista de cadenas y procésalas para cumplir con el objetivo mencionado realizando las siguientes actividades:\n",
    "\n",
    "1. Crea un RDD con los datos de las frases que se han proporcionado.\n",
    "2. Utiliza la transformación `flatMap` para descomponer las frases en palabras.\n",
    "3. Filtra las palabras para retener solo aquellas con cuatro letras o más.\n",
    "4. Recopila y muestra las palabras filtradas para ver el resultado final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NclEunSfnF_r"
   },
   "source": [
    "**Ejercicio 2**: Eres parte del equipo de operaciones de TI en una gran empresa y tu tarea es monitorear y analizar los logs del servidor para asegurarte de que todos los sistemas funcionan correctamente. Se te ha proporcionado un conjunto de entradas de log y necesitas filtrar aquellos mensajes que indican errores graves o advertencias.\n",
    "\n",
    "Las entradas de log se presentan en el siguiente formato:\n",
    "- \"2024-04-26 12:00:01 INFO Usuario ha iniciado sesión con éxito\"\n",
    "- \"2024-04-26 12:01:15 ERROR Fallo en la base de datos\"\n",
    "- \"2024-04-26 12:02:37 WARNING Memoria casi llena\"\n",
    "\n",
    "Identifica y extrae todos los logs que contienen mensajes de error (ERROR) o advertencias (WARNING).\n",
    "\n",
    "1. Crea un RDD con los datos de los logs que se han proporcionado.\n",
    "2. Utiliza la transformación `flatMap` para descomponer las entradas de log en palabras.\n",
    "3. Filtra las entradas para retener solo aquellas que contienen las palabras 'ERROR' o 'WARNING'.\n",
    "4. Recopila y muestra las entradas filtradas para ver los mensajes críticos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpmZCh5Yo47M"
   },
   "source": [
    "# Tema 3: Agregaciones con `reduceByKey` y `groupBy`\n",
    "\n",
    "En el procesamiento de grandes volúmenes de datos, las operaciones de agregación son esenciales para resumir la información y obtener insights significativos. PySpark ofrece varias transformaciones que facilitan estas tareas de manera eficiente:\n",
    "\n",
    "- **`reduceByKey`**: Esta transformación combina los valores de cada clave utilizando una función reductora especificada. Es ideal para realizar operaciones agregadas como sumas, promedios, o cualquier otro cálculo que se pueda definir de manera asociativa y conmutativa. `reduceByKey` es particularmente útil en datasets grandes, ya que reduce la cantidad de datos transferidos al combinar los resultados localmente en cada partición antes de enviar resultados a través de la red.\n",
    "  \n",
    "- **`groupBy`**: Permite agrupar los datos en el RDD según una función especificada. El resultado es un RDD de pares clave-valor, donde la clave es el resultado de la función `groupBy` y el valor es un iterable de los elementos que comparten esa clave. Aunque `groupBy` puede ser poderoso, es menos eficiente que `reduceByKey` para operaciones de agregación porque realiza toda la agrupación sin combinar datos, lo que puede ser costoso en términos de memoria y tiempo de procesamiento.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc5inJEsprSC"
   },
   "source": [
    "## Ejemplo\n",
    "Consideremos un escenario donde estamos trabajando con un conjunto de datos de ventas de productos. Cada entrada contiene el nombre del producto y la cantidad vendida. Nuestro objetivo es calcular la cantidad total vendida por producto:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JY_9CIN_jWzm",
    "outputId": "6763fa7c-ab38-47bd-db12-4c3c2d980847"
   },
   "outputs": [],
   "source": [
    "# Datos de ejemplo\n",
    "ventas = [(\"manzana\", 2), (\"banana\", 3), (\"manzana\", 1), (\"naranja\", 5), (\"banana\", 1)]\n",
    "\n",
    "# Crear un RDD\n",
    "ventas_rdd = sc.parallelize(ventas)\n",
    "\n",
    "# Usar reduceByKey para sumar las ventas por producto\n",
    "ventas_por_producto = ventas_rdd.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Recoger y mostrar los resultados\n",
    "resultados = ventas_por_producto.collect()\n",
    "print(\"Ventas totales por producto:\")\n",
    "for producto, total in resultados:\n",
    "    print(f\"{producto}: {total}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyWpUCFMqfgZ"
   },
   "source": [
    "**Ejercicio 1**: Eres un analista de datos en una empresa que vende productos electrónicos en línea. La empresa está interesada en entender mejor las ventas de sus productos a lo largo del tiempo para poder ajustar las estrategias de marketing y stock. Se te ha proporcionado un conjunto de datos que contiene las ventas diarias de varios productos. Tu tarea es calcular la venta total por producto para el último mes y determinar cuál ha sido el producto más vendido.\n",
    "\n",
    "Se han registrado las ventas diarias de productos, y cada registro contiene el nombre del producto y la cantidad vendida en ese día. Ejemplos de entradas incluyen:\n",
    "- (\"Smartphone\", 5)\n",
    "- (\"Tablet\", 3)\n",
    "- (\"Laptop\", 2)\n",
    "- (\"Smartphone\", 2)\n",
    "- (\"Laptop\", 3)\n",
    "- (\"Smartwatch\", 1)\n",
    "\n",
    "**Objetivo del Ejercicio**:\n",
    "1. Crea un RDD con los datos de ventas proporcionados.\n",
    "2. Utiliza `reduceByKey` para calcular el total de ventas por producto.\n",
    "3. Encuentra el producto con la mayor cantidad de ventas en el mes.\n",
    "4. Muestra el nombre del producto más vendido y su total de ventas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B_X-t-GuN-g"
   },
   "source": [
    "**Ejercicio 2**: Eres un analista de datos en un sitio web de comercio electrónico y te han pedido que analices el tráfico del sitio para identificar las horas pico de visitas y la distribución del tráfico por categoría de producto. El objetivo es optimizar los recursos del servidor durante las horas de mayor actividad y ajustar las campañas publicitarias según las categorías más populares.\n",
    "\n",
    "El sistema de seguimiento del sitio web genera logs que incluyen la hora exacta de la visita y la categoría del producto que el visitante exploró. Cada entrada de log tiene el siguiente formato:\n",
    "- (timestamp, categoría_producto)\n",
    "- Ejemplos de entradas de log:\n",
    "  - (\"2024-04-26 12:00:00\", \"Electrónicos\")\n",
    "  - (\"2024-04-26 12:05:00\", \"Libros\")\n",
    "  - (\"2024-04-26 12:09:00\", \"Ropa\")\n",
    "  - (\"2024-04-26 12:15:00\", \"Hogar\")\n",
    "  - (\"2024-04-26 13:00:00\", \"Electrónicos\")\n",
    "\n",
    "1. Crea un RDD con los datos de los logs proporcionados.\n",
    "2. Extrae la hora (sin minutos ni segundos) de cada timestamp y agrupa los datos por hora para determinar cuándo ocurren las horas pico.\n",
    "3. Utiliza `reduceByKey` para contar las visitas por cada categoría de producto y determinar cuáles son las más populares.\n",
    "4. Genera un reporte final que muestre las horas pico de visitas y las tres categorías más populares.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0jvkMfngmq0"
   },
   "source": [
    "**Ejercicio 3**: Imagina que eres un analista de datos trabajando para una agencia de marketing digital. Se te ha encargado analizar datos de redes sociales para identificar las tendencias de hashtags más populares que se utilizan en conjunto con ciertas palabras clave.\n",
    "\n",
    "Se te han proporcionado extractos de publicaciones de una red social. Estas publicaciones incluyen una mezcla de texto normal y hashtags. Por ejemplo:\n",
    "- \"Amando la vida en la playa este verano #verano #playa\"\n",
    "- \"¡Increíble tecnología que está cambiando el mundo! #innovación #tecnología\"\n",
    "- \"Todo sobre cómo iniciar un negocio en línea #emprendimiento #negocios\"\n",
    "- \"Descubre los secretos de una dieta saludable #salud #bienestar\"\n",
    "\n",
    "Tu tarea es extraer todos los hashtags y luego identificar aquellos que frecuentemente aparecen en publicaciones que contienen las palabras 'tecnología', 'salud' o 'negocios'.\n",
    "\n",
    "1. Crea un RDD con los datos de las publicaciones.\n",
    "2. Utiliza `flatMap` para descomponer las publicaciones en palabras y extraer los hashtags.\n",
    "3. Filtra las publicaciones para encontrar aquellas que contienen las palabras clave 'tecnología', 'salud' o 'negocios'.\n",
    "4. De estas publicaciones, extrae todos los hashtags y determina cuáles son los más comunes.\n",
    "5. Genera un reporte final que muestre los hashtags más comunes para cada palabra clave.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeQxMF3rxZpZ"
   },
   "source": [
    "# Tema 4: Uniendo RDDs con `join`, `leftOuterJoin`, y `rightOuterJoin`\n",
    "\n",
    "Las operaciones de combinación son fundamentales en el procesamiento de datos para unir diferentes conjuntos de datos basados en claves comunes. PySpark proporciona varias funciones de combinación que permiten unir dos RDDs de formas variadas, dependiendo de los requerimientos del análisis:\n",
    "\n",
    "- **`join`**: Combina dos RDDs donde la clave existe en ambos. El resultado es un RDD de pares clave-valor, donde cada valor es un par que contiene elementos de ambos RDDs para esa clave.\n",
    "- **`leftOuterJoin`**: Combina dos RDDs pero mantiene todos los elementos del RDD de la izquierda, incluso si no hay una correspondencia en el RDD de la derecha. Los elementos del RDD derecho que no tienen una clave correspondiente en el RDD izquierdo aparecerán como `None`.\n",
    "- **`rightOuterJoin`**: Similar al `leftOuterJoin`, pero mantiene todos los elementos del RDD de la derecha, y los elementos del RDD izquierdo sin correspondencia aparecerán como `None`.\n",
    "\n",
    "Estas operaciones son esenciales para análisis donde se necesita integrar información de múltiples fuentes para obtener una vista más completa o para realizar comparaciones y análisis basados en datos agregados de varias tablas o fuentes de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYsa3ezpyEuh"
   },
   "source": [
    "## Ejemplos\n",
    "\n",
    "### Ejemplo 1\n",
    "Consideremos un escenario donde tenemos dos conjuntos de datos: uno con información de ventas de productos y otro con detalles de los productos. Queremos combinar estos datos para obtener un informe completo que incluya el nombre del producto y sus ventas.\n",
    "\n",
    "Los datos de ventas de productos consisten en pares de (producto_id, ventas), y los detalles de los productos en pares de (producto_id, nombre_producto). Nuestro objetivo es usar la función `join` para combinar estos dos RDDs por `producto_id` y así obtener un informe que relacione cada producto con sus respectivas ventas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j9i4DUOMqw_T",
    "outputId": "2f554bc5-cf7d-4431-cfdb-98ed3a6f1400"
   },
   "outputs": [],
   "source": [
    "# Datos de ventas de productos (producto_id, ventas)\n",
    "ventas_rdd = sc.parallelize([(1, 100), (2, 80), (3, 50)])\n",
    "\n",
    "# Datos de detalles de productos (producto_id, nombre_producto)\n",
    "detalles_rdd = sc.parallelize([(1, \"Smartphone\"), (2, \"Tablet\"), (3, \"Laptop\")])\n",
    "\n",
    "# Realizar un join para combinar los datos\n",
    "productos_ventas = ventas_rdd.join(detalles_rdd)\n",
    "\n",
    "# Recoger y mostrar los resultados\n",
    "resultados = productos_ventas.collect()\n",
    "print(\"Reporte de Ventas:\")\n",
    "for id, (ventas, nombre) in resultados:\n",
    "    print(f\"{nombre}: {ventas} unidades vendidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoaakzTxz2bq"
   },
   "source": [
    "### Ejemplo 2\n",
    "En este ejemplo, vamos a simular un entorno donde necesitamos cargar datos desde archivos externos. Supongamos que tenemos dos archivos: uno con ventas de productos y otro con detalles de los productos. Estos archivos están almacenados en formato CSV y queremos leer estos datos en RDDs, luego combinarlos para obtener un informe detallado que relacione las ventas con los nombres de los productos.\n",
    "\n",
    "Los archivos están estructurados de la siguiente manera:\n",
    "- **ventas.csv**: Contiene `producto_id` y `ventas`, separados por comas.\n",
    "- **detalles.csv**: Contiene `producto_id` y `nombre_producto`, separados por comas.\n",
    "\n",
    "El objetivo es leer estos archivos, crear RDDs correspondientes y usar la función `join` para combinar estos RDDs por `producto_id`. Este proceso simulará un análisis real donde se combinan datos de ventas y detalles del producto para generar informes útiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "89hOePmcyMM8",
    "outputId": "82e2b2f5-3c48-4ad2-a662-418683773b6f"
   },
   "outputs": [],
   "source": [
    "# Leer los archivos en RDDs\n",
    "# Suponiendo que los archivos están en el directorio actual donde se ejecuta PySpark\n",
    "ventas_rdd = sc.textFile(\"ventas.csv\")\n",
    "detalles_rdd = sc.textFile(\"detalles.csv\")\n",
    "\n",
    "# Convertir las líneas de CSV a pares (producto_id, valor)\n",
    "ventas_rdd = ventas_rdd.map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"producto_id\").map(lambda x: (int(x[0]), int(x[1])))\n",
    "detalles_rdd = detalles_rdd.map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"producto_id\").map(lambda x: (int(x[0]), x[1]))\n",
    "\n",
    "# Realizar un join para combinar los datos basados en producto_id\n",
    "productos_ventas = ventas_rdd.join(detalles_rdd)\n",
    "\n",
    "# Recoger y mostrar los resultados\n",
    "resultados = productos_ventas.collect()\n",
    "print(\"Reporte de Ventas Combinadas:\")\n",
    "for id, (ventas, nombre) in resultados:\n",
    "    print(f\"Producto: {nombre}, Ventas: {ventas} unidades\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXzTBm204Tgs"
   },
   "source": [
    "### Ejemplo 3\n",
    "Supongamos que estamos trabajando en un análisis para una empresa que desea entender mejor las tendencias de compra y las preferencias de sus clientes. Para ello, disponemos de tres conjuntos de datos: clientes, productos y ventas. Queremos combinar estos datos para obtener un informe detallado que relacione las ventas con los productos y los clientes.\n",
    "\n",
    "- **clientes.csv**: Contiene `cliente_id`, `nombre`, y `region`.\n",
    "- **productos.csv**: Contiene `producto_id`, `nombre_producto`, y `categoria`.\n",
    "- **ventas2.csv**: Contiene `venta_id`, `cliente_id`, `producto_id`, y `cantidad`.\n",
    "\n",
    "Nuestro objetivo es unir estos datos para crear un informe que muestre las preferencias de compra por región y categoría de producto, analizando las ventas y la distribución de los clientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMN48eJn0XJL",
    "outputId": "60d47e95-a573-435a-f459-866ffa880b20"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que los archivos están en el directorio actual donde se ejecuta PySpark\n",
    "# Leer los archivos en RDDs\n",
    "clientes_rdd = sc.textFile(\"clientes.csv\").map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"cliente_id\").map(lambda x: (int(x[0]), (x[1], x[2])))\n",
    "productos_rdd = sc.textFile(\"productos.csv\").map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"producto_id\").map(lambda x: (int(x[0]), (x[1], x[2])))\n",
    "ventas_rdd = sc.textFile(\"ventas2.csv\").map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"venta_id\").map(lambda x: (int(x[1]), (int(x[2]), int(x[3]))))\n",
    "\n",
    "# Unir las ventas con los productos por producto_id\n",
    "ventas_productos = ventas_rdd.map(lambda x: (x[1][0], (x[0], x[1][1]))) # re-map para producto_id como clave\n",
    "venta_detalle_producto = ventas_productos.join(productos_rdd).map(lambda x: (x[1][0][0], (x[0], x[1][0][1], x[1][1][0], x[1][1][1]))) # remap para cliente_id como clave\n",
    "\n",
    "# Unir los resultados con los clientes\n",
    "venta_detalle_final = venta_detalle_producto.join(clientes_rdd)\n",
    "\n",
    "# Recoger y mostrar los resultados\n",
    "resultados = venta_detalle_final.collect()\n",
    "print(\"Reporte Detallado de Ventas:\")\n",
    "for cliente, ((producto_id, cantidad, nombre_producto, categoria), (nombre_cliente, region)) in resultados:\n",
    "    print(f\"Cliente: {nombre_cliente}, Región: {region}, Producto: {nombre_producto}, Categoría: {categoria}, Cantidad: {cantidad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_urUzMI-vfh"
   },
   "source": [
    "## Ejercicios\n",
    "\n",
    "**Ejercicio 1**: Eres un analista de datos en una empresa que vende productos tecnológicos. Se te ha dado la tarea de crear un informe que combine información de inventario de productos con datos de ventas recientes. El objetivo es producir un listado que muestre el nombre del producto junto con las unidades vendidas en el último mes. La información se encuentra en las bases de datos:\n",
    "\n",
    "- **inventario.csv**: Contiene `producto_id` y `nombre_producto`.\n",
    "- **ventas_mensuales.csv**: Contiene `producto_id` y `unidades_vendidas`.\n",
    "\n",
    "\n",
    "1. Cargar los datos de ambos archivos en RDDs.\n",
    "2. Utilizar la función `join` para combinar los datos de inventario con los de ventas basándose en `producto_id`.\n",
    "3. Mostrar el resultado combinado que incluya el nombre del producto y las unidades vendidas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPaBWYsG6ANl"
   },
   "source": [
    "**Ejercicio 2**: Eres un analista de datos trabajando para una compañía que quiere entender mejor la eficacia de su base de datos de clientes en relación con las transacciones realizadas. Algunos registros de transacciones pueden no tener un cliente registrado debido a errores en el proceso de entrada de datos o clientes que no se registraron correctamente. La información se encuentra almacenada en las bases de datos:\n",
    "\n",
    "- **clientes2.csv**: Contiene `cliente_id`, `nombre`, y `region`.\n",
    "- **transacciones.csv**: Contiene `transaccion_id`, `cliente_id`, y `monto`.\n",
    "\n",
    "Se sabe que no todos los `cliente_id` en las transacciones tienen un correspondiente en la base de datos de clientes, y se desea identificar estas transacciones para un análisis posterior.\n",
    "\n",
    "1. Cargar los datos de ambos archivos en RDDs.\n",
    "2. Utilizar `leftOuterJoin` para unir las transacciones con los clientes.\n",
    "3. Identificar y listar todas las transacciones que no tienen un cliente registrado asociado.\n",
    "4. Mostrar el total del monto de las transacciones no registradas y el número de tales casos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I3iaxxgBHVm"
   },
   "source": [
    "---\n",
    "\n",
    "## Anexo: Expresiones Lambda en Python\n",
    "\n",
    "### Definición y Finalidad\n",
    "Las expresiones lambda en Python son pequeñas funciones anónimas definidas con la palabra clave `lambda`. Lambda puede tener cualquier número de argumentos, pero solo puede tener una expresión. Son sintácticamente restringidas a una sola expresión. Se usan ampliamente en situaciones donde se necesita una función por un corto período de tiempo y definirla con la sintaxis normal de función sería excesivo.\n",
    "\n",
    "### Estructura de una Expresión Lambda\n",
    "La sintaxis básica de una expresión lambda es:\n",
    "\n",
    "```python\n",
    "lambda arguments: expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlivTDmH7Lqm",
    "outputId": "026a8cd2-4549-42e6-9eb4-81e5a4987dc6"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función lambda que suma dos números\n",
    "sumar = lambda x, y: x + y\n",
    "print(sumar(3, 5))  # Salida: 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xv5H292iCAHE",
    "outputId": "18e8dfb6-58e0-4ea7-ab14-f7ed4f02b052"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función lambda con operador condicional\n",
    "# Devuelve el mayor de dos números\n",
    "mayor = lambda x, y: x if x > y else y\n",
    "print(mayor(8, 5))  # Salida: 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kCRWiS0CB10",
    "outputId": "6c637d9c-19d4-4f30-e660-28b0c274fc18"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función lambda utilizada con map()\n",
    "# Calcula el cuadrado de cada número en una lista\n",
    "numeros = [1, 2, 3, 4, 5]\n",
    "cuadrados = list(map(lambda x: x**2, numeros))\n",
    "print(cuadrados)  # Salida: [1, 4, 9, 16, 25]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vj632RASCHAJ",
    "outputId": "be63fc16-7376-4afc-a62e-0359d8d10cb0"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de función lambda utilizada con filter()\n",
    "# Filtra y devuelve solo los números pares de una lista\n",
    "numeros = [1, 2, 3, 4, 5]\n",
    "pares = list(filter(lambda x: x % 2 == 0, numeros))\n",
    "print(pares)  # Salida: [2, 4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53Rn3x0rCI0Z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
