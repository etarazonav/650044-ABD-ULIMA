{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etarazonav/650044-ABD-ULIMA/blob/main/Notebooks/ABB_SparkSQ4_Funciones_adicionales_fechas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img style=\"float: left; padding: 0px 10px 0px 0px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Universidad_de_Lima_logo.png/220px-Universidad_de_Lima_logo.png\"  width=\"120\" />  Parte 4: Funciones Adicionales y Fechas\n",
        "**Profesor:** Enver G. Tarazona Vargas <br>\n",
        "**Curso:** Analítica con Big Data <br>\n",
        "**FACULTAD DE INGENIERÍA - CARRERA DE INGENIERÍA DE SISTEMAS**<br>"
      ],
      "metadata": {
        "id": "MWVUlFFyM38l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKtr6MK0Kegl"
      },
      "source": [
        "!pip install -q pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TQPqbw4KeOk"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de archivos\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/ventas.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/covid/covid.csv"
      ],
      "metadata": {
        "id": "JpJ-UkRlwmHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR0JlQgCKLFW"
      },
      "source": [
        "df = spark.read.csv('ventas.csv', inferSchema=True, header=True)\n",
        "df.show(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOyC5hjtMmQf"
      },
      "source": [
        "## 1.&nbsp;Funciones\n",
        "\n",
        "Más información en: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OBHlFbZKLFc"
      },
      "source": [
        "# Ejemplos de funciones\n",
        "from pyspark.sql.functions import avg, stddev, countDistinct, round"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conteo de elementos distintos"
      ],
      "metadata": {
        "id": "60jSkhcrFQHZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "523gZe1HNCET"
      },
      "source": [
        "df.select(countDistinct(\"Compania\")).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Operaciones estadísticas como `stddev`, `avg`. Se puede utilizar `alias` para asignar un nombre más adecuado a la columna resultante y `round` para redondear."
      ],
      "metadata": {
        "id": "dmM5wkzsFbKy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6GScfIONT_b"
      },
      "source": [
        "# Desviación estándar de las ventas\n",
        "df.select(round(stddev(\"Ventas\"), 2).alias(\"Desv ventas\"),\n",
        "          round(avg('Ventas'),2).alias('Media ventas')\n",
        "          ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el manejo de cadenas de caracteres se tiene algunas de las siguientes funciones"
      ],
      "metadata": {
        "id": "Gl3kJyrzHxFJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sg0o4MTei6lf"
      },
      "source": [
        "from pyspark.sql.functions import concat, substring, length, trim, lower, upper, replace\n",
        "\n",
        "df.select(concat(\"Compania\", \"Persona\").alias(\"Concat\"),\n",
        "          substring(\"Compania\", 1, 3).alias(\"Substring\"),\n",
        "          length(\"Compania\").alias(\"Longitud\"),\n",
        "          lower(\"Compania\").alias(\"minúscula\"),\n",
        "          upper(\"Compania\").alias(\"mayúscula\"),\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunas funciones matemáticas aplicables por columna son las siguientes"
      ],
      "metadata": {
        "id": "IPWfMwrCK4tF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iQeUKisKLFb"
      },
      "source": [
        "from pyspark.sql.functions import abs, round, ceil, floor, exp, log\n",
        "\n",
        "df.select(abs(\"Ventas\").alias(\"Valor_absoluto\"),\n",
        "          round(\"Ventas\", 2).alias(\"Redondeo\"),\n",
        "          ceil(\"Ventas\").alias(\"Ceil\"),\n",
        "          floor(\"Ventas\").alias(\"Floor\"),\n",
        "          exp(\"Ventas\").alias(\"Exp\"),\n",
        "          round(log(\"Ventas\"),5).alias(\"Log\")\n",
        "          ).show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La instrucción `when` asigna un valor condicionalmente"
      ],
      "metadata": {
        "id": "_4WRn0A5M8Cp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsAOJh6svf7m"
      },
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "df.select(round(df['Ventas']),\n",
        "          when(df['Ventas'] >= 500, 'A').otherwise('B').alias('reemplazo'),\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAymxBnNML8e"
      },
      "source": [
        "from pyspark.sql.functions import when, col\n",
        "\n",
        "df.withColumn('Compania_num',\n",
        "              when(col('Compania') == 'Guguel', 0)\n",
        "              .when(col('Compania') == 'Feisbuk', 1)\n",
        "              .when(col('Compania') == 'Maikrosof', 2)\n",
        "              .otherwise(3)\n",
        ").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.&nbsp;Fechas\n",
        "\n"
      ],
      "metadata": {
        "id": "8U5HT30SPAG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('covid.csv', inferSchema=True, header=True)\n",
        "df.show(4)"
      ],
      "metadata": {
        "id": "4gIjMQP3PHaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(df['fecha_ingreso_hosp'] ).show(5)"
      ],
      "metadata": {
        "id": "FxZuT0lJPHr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se debe adecuar el formato a `DateTime`. Para realizar esta conversión se puede usar la función `to_date`. Para más información sobre los formatos ver: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html"
      ],
      "metadata": {
        "id": "tCfZLzPrTKNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Creación de una columna llamada \"ingreso\" con formato de fecha \"adecuado\"\n",
        "df = df.withColumn('ingreso', to_date(df['fecha_ingreso_hosp'], 'd/M/yyyy'))\n",
        "\n",
        "df.select(['fecha_ingreso_hosp', 'ingreso']).show(5)"
      ],
      "metadata": {
        "id": "XxWfIlsES_X6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import year, month, dayofmonth, dayofyear, weekofyear\n",
        "\n",
        "# Mostrar año, mes, día, semana, por separado\n",
        "df.select(year(df['ingreso']).alias(\"año\"),\n",
        "          month(df['ingreso']).alias(\"mes\"),\n",
        "          dayofmonth(df['ingreso']).alias(\"día\"),\n",
        "          weekofyear(df['ingreso']).alias(\"semana\"),\n",
        "          dayofyear(df['ingreso']).alias(\"día del año\")\n",
        "          ).show(5)"
      ],
      "metadata": {
        "id": "TCMtE8QbPHn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXd9YWshssj7"
      },
      "source": [
        "### Ejemplo: agrupamiento con fechas\n",
        "\n",
        "Se desea conocer qué años están presentes en el conjunto de datos. Se usará `distinct` para recuperar los valores distintos de años"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(year(df['ingreso'])).distinct().show()"
      ],
      "metadata": {
        "id": "TZRZGl5WWN83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea saber cuántos ingresos ocurrieron en cada año"
      ],
      "metadata": {
        "id": "nsZ0slTxWtpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(year(df['ingreso'])).count().show()"
      ],
      "metadata": {
        "id": "O-srWfm_WxMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea analizar en qué mes ocurrió la mayor cantidad de ingresos, independientemente del año"
      ],
      "metadata": {
        "id": "z3Nv2BvtW9-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(month(df['ingreso']).alias('mes')) \\\n",
        "  .count() \\\n",
        "  .withColumnRenamed('count', 'total') \\\n",
        "  .orderBy('mes') \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "dx553xrhXDQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se desea saber cuántos internamientos existen por año y por departamento de domicilio, y mostrar los 10 primeros ordenados de mayor a menor"
      ],
      "metadata": {
        "id": "mtZdPWlwZXjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "df.groupBy(year(df['ingreso']), df['dep_domicilio']) \\\n",
        "  .count() \\\n",
        "  .orderBy(desc('count')) \\\n",
        "  .show(10)"
      ],
      "metadata": {
        "id": "A-QCtrLPYbhF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}