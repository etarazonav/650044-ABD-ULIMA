{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etarazonav/650044-ABD-ULIMA/blob/main/Notebooks/ABB_SparkSQ2_Consultas_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img style=\"float: left; padding: 0px 10px 0px 0px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Universidad_de_Lima_logo.png/220px-Universidad_de_Lima_logo.png\"  width=\"120\" /> Parte 2: Uso de consultas SQL\n",
        "\n",
        "**Profesor:** Enver G. Tarazona Vargas <br>\n",
        "**Curso:** Analítica con Big Data <br>\n",
        "**FACULTAD DE INGENIERÍA - CARRERA DE INGENIERÍA DE SISTEMAS**<br>\n"
      ],
      "metadata": {
        "id": "J9sjK9z51f9-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La documentación sobre el API de SQL en Spark es: https://spark.apache.org/docs/latest/sql-ref.html"
      ],
      "metadata": {
        "id": "MXladRMCLPdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyspark"
      ],
      "metadata": {
        "id": "iCMXki2GLSXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "metadata": {
        "id": "q_VpbkBGWCt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de archivos\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/personas.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/poema.txt\n"
      ],
      "metadata": {
        "id": "6I_7NOmWj8Ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1: A partir de un DataFrame\n",
        "\n",
        "**Creación de un DataFrame**"
      ],
      "metadata": {
        "id": "ql8bQuSsY-ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.load(\"personas.csv\",\n",
        "                     format=\"csv\", sep=\";\", inferSchema=True, header=True)\n",
        "df.show(2)"
      ],
      "metadata": {
        "id": "-U0tGBbKWIeF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vista: \"Creación de una tabla\"**"
      ],
      "metadata": {
        "id": "F3kr1f2PZET8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para poder realizar consultas (\"queries\") usando SQL sobre un DataFrame, primero se debe crear una vista temporal del DataFrame usando `createOrReplaceTempView`. Luego sobre esta vista temporal se realiza las consultas usando SQL."
      ],
      "metadata": {
        "id": "NNmJymqOLVs9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una \"tabla\" llamada \"tabla1\" con los datos del dataframe df\n",
        "df.createOrReplaceTempView('tabla1')"
      ],
      "metadata": {
        "id": "IOItW3D0LTLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejemplos de consultas:**\n",
        "\n",
        "Se puede realizar consultas de tipo SQL sobre la vista temporal creada, que en este caso se llama `tabla1`. El resultado es un DataFrame de PySpark."
      ],
      "metadata": {
        "id": "GxlxNwi2Y6K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM tabla1\")"
      ],
      "metadata": {
        "id": "kbHgiDOyX5HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para visualizar el resultado se puede colocar directamente `show`, o se puede almacenar en una variable (DataFrame) y visualizar dicho DataFrame usando `show`."
      ],
      "metadata": {
        "id": "7_mEs2yLZVsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = spark.sql(\"SELECT * FROM tabla1\")\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "CFjnXu1yXwPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM tabla1 ORDER BY edad DESC LIMIT 2\").show()"
      ],
      "metadata": {
        "id": "WV7JFBMLYluF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM tabla1 WHERE edad<26\").show()"
      ],
      "metadata": {
        "id": "gj-my59jLTH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM tabla1 WHERE nombre LIKE 'Jorge'\").show()"
      ],
      "metadata": {
        "id": "pmaEx_gOLTCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT count(*) AS Elementos from tabla1\").show()"
      ],
      "metadata": {
        "id": "lCTk3Ktkf0gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creación de UDF (User Defined Function)**"
      ],
      "metadata": {
        "id": "Lc-Z5CX1gHWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "7anuWYq2gH0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf(\"integer\")\n",
        "def suma_cien(s: pd.Series) -> pd.Series:\n",
        "    return s + 100\n",
        "\n",
        "spark.udf.register(\"suma_cien\", suma_cien)"
      ],
      "metadata": {
        "id": "LHOQD8GAgzjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uso del UDF para una consulta sobre los datos usando SQL."
      ],
      "metadata": {
        "id": "hFWu7SMSgsqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT nombre, suma_cien(edad) AS edad100 FROM tabla1\").show()"
      ],
      "metadata": {
        "id": "9zd2PMrYgHvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "df.selectExpr('suma_cien(edad)').show()"
      ],
      "metadata": {
        "id": "5PKRNA58gHod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2: A partir de un RDD\n",
        "\n",
        "Si la entrada de datos utiliza un RDD, primero se debe convertir a un DataFrame y luego recién a una vista temporal para poder realizar consultas SQL sobre dicha vista temporal."
      ],
      "metadata": {
        "id": "49KoY_7iLdzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "\n",
        "# Creación de un RDD a partir de un archivo de texto\n",
        "rdd = sc.textFile(\"poema.txt\")\n",
        "rdd.take(3)"
      ],
      "metadata": {
        "id": "DZStfl7xa07c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Se importa re para usar expresiones regulares\n",
        "import re\n",
        "\n",
        "# Contar palabras usando el esquema MapReduce\n",
        "rdd2 = rdd.flatMap(lambda x: x.lower().split())\\\n",
        "          .map(lambda x: (re.sub(r'\\W+', '', x), 1)) \\\n",
        "          .reduceByKey(lambda x,y: x+y)\n",
        "\n",
        "rdd2.take(10)"
      ],
      "metadata": {
        "id": "kk9HsNaybrkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Uso de Expresiones Regulares con la transformación `map`\n",
        "1. **`map(lambda x: ...)`**:\n",
        "   - `map` es una operación en Spark que transforma cada elemento del RDD aplicando la función dada (en este caso, la función anónima `lambda x: ...`). Aquí, cada palabra que viene de la operación anterior en el RDD será el valor de `x`.\n",
        "\n",
        "2. **`(re.sub(r'\\W+', '', x), 1)`**:\n",
        "   - **`re.sub(r'\\W+', '', x)`**: Esto usa la función `re.sub` de la biblioteca `re` (que maneja expresiones regulares en Python). Esta función toma tres argumentos:\n",
        "     1. **`r'\\W+'`**: Esta es la expresión regular. Desglosemosla:\n",
        "        - `\\W` significa cualquier carácter que *no* sea una letra, número o guion bajo.\n",
        "        - El `+` indica que esta búsqueda será para una o más de esas coincidencias consecutivas.\n",
        "     2. **`''`**: Es el valor de reemplazo, que en este caso es una cadena vacía. Significa que todo lo que coincida con la expresión regular se eliminará.\n",
        "     3. **`x`**: Es el texto o palabra que se está limpiando.\n",
        "\n",
        "     Por lo tanto, la función `re.sub(r'\\W+', '', x)` reemplaza cualquier secuencia de caracteres no alfanuméricos (espacios, puntuación, símbolos) en la palabra `x` con una cadena vacía, esencialmente eliminando estos caracteres y dejando solo letras y números.\n",
        "\n",
        "   - **Ejemplo**:\n",
        "     - Si `x = 'Hola, mundo!'`, la expresión **`re.sub(r'\\W+', '', x)`** convertirá la palabra en `'Holamundo'` (eliminando la coma y el signo de exclamación).\n",
        "   \n",
        "3. **`, 1`**:\n",
        "   - Después de limpiar la palabra con `re.sub`, se devuelve un par `(palabra_limpia, 1)`.\n",
        "   - El `1` es simplemente un marcador que más adelante se utilizará para contar cuántas veces aparece cada palabra.\n",
        "\n",
        "### Ejemplo paso a paso:\n",
        "\n",
        "- Supongamos que la palabra original es `\"¡Hola!\"`.\n",
        "- El **`re.sub(r'\\W+', '', \"¡Hola!\")`** elimina todos los caracteres no alfanuméricos, devolviendo `\"Hola\"`.\n",
        "- El resultado de la expresión **`map(lambda x: (re.sub(r'\\W+', '', x), 1))`** sería el par **`('Hola', 1)`**.\n",
        "\n",
        "### Resumen de la expresión:\n",
        "\n",
        "- **`re.sub(r'\\W+', '', x)`**: Limpia cada palabra eliminando todos los caracteres que no sean letras o números.\n",
        "- **`map(lambda x: (re.sub(r'\\W+', '', x), 1))`**: Toma cada palabra limpia y la convierte en un par clave-valor donde la clave es la palabra y el valor es `1`, preparándolo para el conteo de palabras más adelante.\n",
        "\n"
      ],
      "metadata": {
        "id": "fqGjGzt66JRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "# Crear un RDD con filas de tipo \"Row\"\n",
        "rdd3 = rdd2.map(lambda x: Row(palabra=x[0], contador=x[1], longitud=len(x[0])))\n",
        "rdd3.take(3)"
      ],
      "metadata": {
        "id": "RFRRebpNbrX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear un DataFrame a partir del RDD\n",
        "df = spark.createDataFrame(rdd3)\n",
        "\n",
        "df.show(4)"
      ],
      "metadata": {
        "id": "IVun9HKkbrQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una vista temporal\n",
        "df.createOrReplaceTempView(\"tabla2\")\n",
        "\n",
        "# Consultas SQL en la tabla\n",
        "spark.sql(\"SELECT palabra, longitud FROM tabla2 WHERE contador>=3 ORDER BY longitud DESC\").show(10)"
      ],
      "metadata": {
        "id": "RpNsjbDDLS-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}