{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etarazonav/650044-ABD-ULIMA/blob/main/Notebooks/ABB_SparkSQ3_Operaciones_Basicas_DataFrames.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img style=\"float: left; padding: 0px 10px 0px 0px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Universidad_de_Lima_logo.png/220px-Universidad_de_Lima_logo.png\"  width=\"120\" /> Parte 3: Operaciones Básicas con DataFrames\n",
        "**Profesor:** Enver G. Tarazona Vargas <br>\n",
        "**Curso:** Analítica con Big Data <br>\n",
        "**FACULTAD DE INGENIERÍA - CARRERA DE INGENIERÍA DE SISTEMAS**<br>"
      ],
      "metadata": {
        "id": "SmQ-1zSUBiJT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0ZqvLuyG7aX"
      },
      "source": [
        "!pip install -q pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m22bpF5HG_zK"
      },
      "source": [
        "# Crear una sesión de Spark (si se corre usando spark-submit o con Google Colab)\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Carga de archivos\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/remuneracion/remuneracion_bruta_2021.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/ventas.csv"
      ],
      "metadata": {
        "id": "KDTVNWRou2YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos que se va a utilizar provienen de la plataforma de [datos abiertos](https://www.datosabiertos.gob.pe/dataset/promedio-mensual-de-remuneraciones-brutas-soles-s-de-trabajadores-en-el-sector-privado-por-0) de Perú y representan el promedio mensual de remuneraciones brutas (en soles) de trabajadores en el sector privado por situación educativa, según distritos, para el año 2021."
      ],
      "metadata": {
        "id": "LRGXM1UmFgRE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cAmt21hGylM"
      },
      "source": [
        "# Lectura de los datos:\n",
        "df_full = spark.read.csv('remuneracion_bruta_2021.csv', inferSchema=True, header=True)\n",
        "\n",
        "# Mostrar el esquema inferido\n",
        "# df_full.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91QvHsTDHSnY"
      },
      "source": [
        "# Mostrar las 5 primeras filas\n",
        "df_full.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.&nbsp;Preprocesamiento"
      ],
      "metadata": {
        "id": "GiCGISbiFT-_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1. Selección de Columnas\n",
        "\n",
        "Dado que el conjunto de datos posee varias columnas, por facilidad se trabajará solo con algunas de ellas. Se seleccionará solo algunas columnas representativas."
      ],
      "metadata": {
        "id": "cTFrWDRaGqGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_full.select(['DISTRITOS', \"EDUCACIÓN SECUNDARIA COMPLETA\", 'GRADO DE BACHILLER', \"TITULADO\", \"GRADO DE MAESTRÍA\", ])\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "LP6Qac55GBjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Conversión de tipo de datos\n",
        "\n",
        "Si uno revisa el esquema de los datos, las columnas numéricas han sido inferidas como `strings`. Para convertir estas columnas a entero se utilizará `withColumn`, para crear una nueva columna, y `cast(\"int\")` para que dicha nueva columna tenga el tipo de datos deseado (entero)."
      ],
      "metadata": {
        "id": "0wp4IUcHGmPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col"
      ],
      "metadata": {
        "id": "EE_iPRkgvXdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nota: esto no dará error pero ignorará los miles (donde hay espacio)\n",
        "\n",
        "# df = df.withColumn(\"secundaria\", col(\"EDUCACIÓN SECUNDARIA COMPLETA\").cast(\"int\"))\n",
        "# df.show(5)"
      ],
      "metadata": {
        "id": "QrT4J7ItJy6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a que en este conjunto de datos se tiene los miles separados por espacio, para poder convertir adecuadamente primero se debe eliminar dicho espacio. Para esto se puede utilizar la función `regex_replace` y luego aplicar el `cast`."
      ],
      "metadata": {
        "id": "p16yD87ILGkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, initcap\n",
        "\n",
        "df.withColumn(\"secundaria\", regexp_replace(\"EDUCACIÓN SECUNDARIA COMPLETA\", \" \", \"\").cast(\"int\")).show(5)"
      ],
      "metadata": {
        "id": "H8gAw2BuKO9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se continuará la aplicación para todas las columnas."
      ],
      "metadata": {
        "id": "jbTSSN40LkpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear columnas numéricas (notar que \"titulado\" se sobrescribe)\n",
        "df = df.withColumn(\"secundaria\", regexp_replace(\"EDUCACIÓN SECUNDARIA COMPLETA\", \" \", \"\").cast(\"int\"))\n",
        "df = df.withColumn(\"bachiller\", regexp_replace(\"GRADO DE BACHILLER\", \" \", \"\").cast(\"int\"))\n",
        "df = df.withColumn(\"titulado\", regexp_replace(\"TITULADO\", \" \", \"\").cast(\"int\"))\n",
        "df = df.withColumn(\"maestria\", regexp_replace(\"GRADO DE MAESTRÍA\", \" \", \"\").cast(\"int\"))\n",
        "\n",
        "# Cambiar las mayúsculas de los distritos a solo mayúscula la primera letra\n",
        "df = df.withColumn(\"distritos\", initcap(col(\"DISTRITOS\")))\n",
        "\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "KXr69om4HuaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se seleccionará solo las columnas convertidas a entero."
      ],
      "metadata": {
        "id": "Z4cNYIbIMCIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select([\"distritos\", \"secundaria\", \"bachiller\", \"titulado\", \"maestria\"])\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "YOrVALe4ILGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.&nbsp;Ordenamiento de resultados: orderBy\n",
        "\n",
        "Se utiliza `orderBy` con el nombre de una columna. Por defecto realiza un ordenamiento ascendente."
      ],
      "metadata": {
        "id": "TUd2zlCwO1Q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy(\"titulado\").show(5)"
      ],
      "metadata": {
        "id": "CtY4FV-fO1wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el caso anterior no se muestra adecuadamente debido a los `NULL` existentes en la columna deseada. Para evitar mostrar estos valores nulos, se puede agregar las siguientes opciones:\n",
        "* `asc_nulls_last`: para order ascendente (mostrando nulos al final)\n",
        "* `desc_nulls_last`: para orden descendente (mostrando nulos al final)\n",
        "\n",
        "Si no hubiesen nulos se podría usar directamente las opciones `asc` y `desc`."
      ],
      "metadata": {
        "id": "jF0dwf60P_0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy(df[\"titulado\"].asc_nulls_last()).show(5)"
      ],
      "metadata": {
        "id": "LAo6O_-YO2Km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy(df[\"titulado\"].desc_nulls_last()).show(5)"
      ],
      "metadata": {
        "id": "hM6G9U55Qr_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EpyopmyfAP3"
      },
      "source": [
        "## 3.&nbsp;Filtraje de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Filtraje con una sola condición (una sola columna)"
      ],
      "metadata": {
        "id": "BJL8rmGXNSNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\"bachiller > 10000\").show(5)"
      ],
      "metadata": {
        "id": "nIrKRIVqEORw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"bachiller\"] > 10000).show(5)"
      ],
      "metadata": {
        "id": "atDx5yACNwZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"distritos\"] == \"San Isidro\").show()"
      ],
      "metadata": {
        "id": "SzosyGWuS8bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"distritos\"].isin([\"San Isidro\", \"Miraflores\", \"Barranco\"]) ).show()"
      ],
      "metadata": {
        "id": "IvRVsFKYUM_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"distritos\"].startswith(\"S\")).show(4, truncate=False)"
      ],
      "metadata": {
        "id": "BMr5yJa_Uuwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"distritos\"].like(\"San %\")).show(5, truncate=False)"
      ],
      "metadata": {
        "id": "7rUN4vhSU_Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenar el resultado\n",
        "df.filter(df[\"bachiller\"] > 10000)\\\n",
        "  .orderBy(\"bachiller\").show(5)"
      ],
      "metadata": {
        "id": "FGWXC_6jOW10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar solo algunas columnas\n",
        "df.filter(df[\"bachiller\"] > 10000) \\\n",
        "  .select([\"distritos\", \"bachiller\"]).show(5)"
      ],
      "metadata": {
        "id": "ewC-cPFhOEox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Filtraje con varias condiciones"
      ],
      "metadata": {
        "id": "E83xOhxwRKDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para  varias condiciones se puede utilizar los operadores lógicos `&`, `|`."
      ],
      "metadata": {
        "id": "X2ntymgcRfLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dos condiciones usando Y lógico\n",
        "\n",
        "df.filter((df[\"secundaria\"]>10000) &\n",
        "          (df[\"bachiller\"]>10000)).show(5)"
      ],
      "metadata": {
        "id": "ijX-XLxGRdnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 condiciones usando OR lógico y NOT lógico\n",
        "\n",
        "df.filter((df[\"secundaria\"]>10000) |\n",
        "          ~(df[\"maestria\"]<10000)).show(5)"
      ],
      "metadata": {
        "id": "a6nD0KVHSPou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.&nbsp;Mezcla de datos"
      ],
      "metadata": {
        "id": "WQyy0L3-Xj7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1. Apilamiento de filas"
      ],
      "metadata": {
        "id": "YOA5cG_SX06M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si se tiene dos o más DataFrames que poseen igual estructura, estos pueden ser apilados para formar un solo DataFrame usando `unionAll`."
      ],
      "metadata": {
        "id": "rMvOPWYSlEbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "row = Row(\"nombre\", \"mascota\", \"cantidad\")"
      ],
      "metadata": {
        "id": "rWTyHgp9mFeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplos de dataframes con similar estructura\n",
        "df1 = spark.createDataFrame([row(\"Susana\", \"gato\", 6),\n",
        "                             row(\"Carlos\", \"perro\", 1),\n",
        "                             row(\"Alberto\", \"pez\", 5)\n",
        "                             ])\n",
        "\n",
        "df2 = spark.createDataFrame([row(\"Pedro\", \"gato\", 2),\n",
        "                             row(\"Carla\", \"tortuga\", 1),\n",
        "                             row(\"Marcos\", \"hamster\", 3)\n",
        "                             ])"
      ],
      "metadata": {
        "id": "7trO6t7VXvr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df1.unionAll(df2)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "LFCAHrzcXvod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2. Joins"
      ],
      "metadata": {
        "id": "jO7xSIRBm_FW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de dataframes\n",
        "row1 = Row(\"nombre\", \"mascota1\", \"cuenta1\")\n",
        "df1 = spark.createDataFrame([row1(\"Susana\", \"gato\", 6),\n",
        "                             row1(\"Carlos\", \"perro\", 1),\n",
        "                             row1(\"Roberto\", \"pez\", 5),\n",
        "                             row1(\"Liliana\", \"caballo\", 1)\n",
        "                             ])\n",
        "\n",
        "row2 = Row(\"nombre\", \"mascota2\", \"cuenta2\")\n",
        "df2 = spark.createDataFrame([row2(\"Susana\", \"loro\", 2),\n",
        "                             row2(\"Carlos\", \"tortuga\", 1),\n",
        "                             row2(\"Roberto\", \"hamster\", 3),\n",
        "                             row2(\"Fernando\", \"pez\", 12)\n",
        "                             ])\n",
        "\n",
        "df1.show()\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "aD1q9cCrXvli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inner Join**\n",
        "\n",
        "Un \"inner join\" realiza la mezcla de filas que tienen correspondencia en ambos DataFrames y elimina todas las otras filas. Esta es la forma por defecto de realizar el join en Spark.\n",
        "\n",
        "En este ejemplo, se realizará el \"inner join\" usando la columna `nombre`."
      ],
      "metadata": {
        "id": "tAzcNARpoUFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, 'nombre', how='inner').show()"
      ],
      "metadata": {
        "id": "VoZLc9vTXvi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si los datasets no tuviesen el mismo nombre de columna, se puede especificar explícitamente las columnas para las cuales se usará un join. En este caso se mantendrá cada columna por separado (en este ejemplo habrá 2 columnas `nombre`, cada una correspondiendo a un dataframe distinto)"
      ],
      "metadata": {
        "id": "gmu_VjFYrnGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, df1[\"nombre\"]==df2[\"nombre\"], how='inner').show()"
      ],
      "metadata": {
        "id": "vEW6KSHrrmKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Outer Join**\n",
        "\n",
        " USa todas las filas (registros) de ambos DataFrames, independientemente de si hay correspondencias o no, y completa los valores faltantes con nulos."
      ],
      "metadata": {
        "id": "sGgXOg1QpAOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, 'nombre', how='outer').show()"
      ],
      "metadata": {
        "id": "q4NA27vXXvgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Left Join**\n",
        "\n",
        "Usa todas las claves del DataFrame de la izquierda. Los datos del DataFrame de la derecha solo aparecen si existe alguna coincidencia con los de la izquierda.\n",
        "\n",
        "En este ejemplo, el DataFrame de la izquierda es el `df1`."
      ],
      "metadata": {
        "id": "_7Fgf757pVHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.join(df2, 'nombre', how='left').show()"
      ],
      "metadata": {
        "id": "_7fycqD1Xvdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.&nbsp;Agregación de datos\n",
        "\n",
        "En esta parte se trabajará con los siguientes datos."
      ],
      "metadata": {
        "id": "mU3mH_rmsfbO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv('ventas.csv', inferSchema=True, header=True)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "gktCk4eFuseo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. Agregación por columnas: agg\n",
        "\n",
        "Una forma de realizar agregación de datos por columnas es utilizando `agg` con alguna de las dos sintaxis mostradas a continuación:\n",
        "* Usando un diccionario (no requiere importar funciones adicionales)\n",
        "* Usando funciones disponibles en `pyspark.sql.functions` (requiere importar las funciones)"
      ],
      "metadata": {
        "id": "aSEZWYdVuebu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma 1: usando un diccionario\n",
        "\n",
        "df.agg({'Ventas':'sum'}).show()\n",
        "# df.agg({'Ventas':'max'}).show()"
      ],
      "metadata": {
        "id": "3wORi-mnXvbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma 2: usando funciones específicas\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "df.agg(sum(\"Ventas\")).show()"
      ],
      "metadata": {
        "id": "65zfNCMywxQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suele ser conveniente utilizar `alias` para modificar el nombre de la columna resultante"
      ],
      "metadata": {
        "id": "nw4ffnj5wK9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.agg(sum(\"Ventas\").alias(\"Ventas totales\")).show()"
      ],
      "metadata": {
        "id": "NMFeXDPxXvD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otras agregaciones usuales son las siguientes (se puede revisar las funciones disponibles en: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n"
      ],
      "metadata": {
        "id": "XbF-gqFMxaHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import count, avg, min, max, stddev, variance\n",
        "\n",
        "df.agg(count(\"Ventas\").alias(\"cuenta\"),\n",
        "       avg(\"Ventas\").alias(\"promedio\"),\n",
        "       min(\"Ventas\").alias(\"mínimo\"),\n",
        "       max(\"Ventas\").alias(\"máximo\"),\n",
        "       stddev(\"Ventas\").alias(\"desviación\"),\n",
        "       variance(\"Ventas\").alias(\"varianza\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "V6ZdUVF5xe99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede redondear el resultado usando la función `round`."
      ],
      "metadata": {
        "id": "lRQ4G0VAzoWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import round\n",
        "\n",
        "df.agg(count(\"Ventas\").alias(\"cuenta\"),\n",
        "       round(avg(\"Ventas\"),2).alias(\"promedio\"),\n",
        "       min(\"Ventas\").alias(\"mínimo\"),\n",
        "       max(\"Ventas\").alias(\"máximo\"),\n",
        "       round(stddev(\"Ventas\"),2).alias(\"desviación\"),\n",
        "       round(variance(\"Ventas\"),2).alias(\"varianza\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "Zeb-JjRgzgBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si se desea recuperar el valor del resultado se puede utilizar `collect`"
      ],
      "metadata": {
        "id": "jcf3mOlwvJ9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = df.agg({'Ventas':'max'}).collect()\n",
        "print(\"Resultado: \", v[0][0])"
      ],
      "metadata": {
        "id": "5T6BJWriXvYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. Agrupamiento usando GroupBy"
      ],
      "metadata": {
        "id": "kLOR75sCz8p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una alternativa para trabajar con datos agrupados es utilizar `groupBy` y algunas funciones usuales que son provistas, como `count`, `mean`, `max`, `min`, `sum`, etc."
      ],
      "metadata": {
        "id": "3hXa8F_w1r4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicación a todas las columnas\n",
        "df.groupBy(\"Compania\") \\\n",
        "  .sum().show()"
      ],
      "metadata": {
        "id": "12TTGacWz79H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.groupBy(\"Compania\").max().show()\n",
        "# df.groupBy(\"Compania\").min().show()\n",
        "# df.groupBy(\"Compania\").count().show()\n",
        "# df.groupBy(\"Compania\").mean().show()"
      ],
      "metadata": {
        "id": "dD-UV6mCwUO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicación a columnas específicas\n",
        "df.groupBy(\"Compania\") \\\n",
        "  .sum(\"Meses\").show()"
      ],
      "metadata": {
        "id": "UFVL_6gV3-HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alternativamente, se puede utilizar `agg` sobre un DataFrame con datos agrupados."
      ],
      "metadata": {
        "id": "gZ8heRbR19DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Compania\") \\\n",
        "  .agg({\"Ventas\":'max'}).show()"
      ],
      "metadata": {
        "id": "hARIb7_hsdqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Compania\") \\\n",
        "  .agg(max(\"Ventas\")).show()"
      ],
      "metadata": {
        "id": "TwaYcJRosdnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Compania\") \\\n",
        "  .agg(sum(\"Ventas\").alias(\"ventas_totales\"), \\\n",
        "       round(avg(\"Meses\"),2).alias(\"meses_promedio\")\n",
        "       ) \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "IN-ZYd_84Omp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se puede añadir condiciones usando `where`"
      ],
      "metadata": {
        "id": "VmPqKKgf48me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Compania\") \\\n",
        "  .agg(sum(\"Ventas\").alias(\"ventas_totales\"), \\\n",
        "       round(avg(\"Meses\"),2).alias(\"meses_promedio\")\n",
        "       ) \\\n",
        "  .where(col(\"ventas_totales\") >= 1000) \\\n",
        "  .show()"
      ],
      "metadata": {
        "id": "1QeZretKsdlS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}