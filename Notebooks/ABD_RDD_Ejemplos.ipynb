{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/etarazonav/650044-ABD-ULIMA/blob/main/Notebooks/ABD_RDD_Ejemplos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <img style=\"float: left; padding: 0px 10px 0px 0px;\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a0/Universidad_de_Lima_logo.png/220px-Universidad_de_Lima_logo.png\"  width=\"120\" /> Ejemplos de análisis de datos usando Spark RDDs\n",
        "**Profesor:** Enver G. Tarazona Vargas <br>\n",
        "**Curso:** Analítica con Big Data <br>\n",
        "**FACULTAD DE INGENIERÍA - CARRERA DE INGENIERÍA DE SISTEMAS**<br>"
      ],
      "metadata": {
        "id": "11jKq_XtXxma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00-TLbxJIkaR"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOPj9bsdIszM"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local[1]\", \"ejemplos_analisis_rdd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weRpEQiWVJrb"
      },
      "source": [
        "Se descargará los archivos que se requiere para estos ejemplos. Alternativamente, pueden ser cargados manualmente. Si se trabaja en un clúster, estos archivos deberían encontrarse en el clúster, por ejemplo de HDFS, y se debe brindar la ruta hacia los archivos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqT2U4GtVIhH"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/texto-prueba.txt\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/DowJones19.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/u.data\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/ventas_rdd.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/detalles_rdd.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/clientes_rdd.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/productos_rdd.csv\n",
        "!wget -q https://raw.githubusercontent.com/etarazonav/650044-ABD-ULIMA/refs/heads/main/Datos/ventas2_rdd.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOIyjX04accI"
      },
      "source": [
        "### **Ejemplo 1: Procesamiento de Texto con RDDs en Apache Spark**\n",
        "\n",
        "**Objetivo**:\n",
        "\n",
        "El objetivo de este ejercicio es procesar un archivo de texto utilizando Apache Spark y sus RDDs (Resilient Distributed Datasets). A través de varias transformaciones, se trabajará con el contenido del archivo, convirtiendo las líneas en palabras, eliminando palabras comunes (stopwords), filtrando por letras iniciales, eliminando duplicados y almacenando los resultados.\n",
        "\n",
        "**Enunciado**:\n",
        "\n",
        "1. **Carga del archivo de texto**: Carga el archivo `texto-prueba.txt` en un RDD utilizando Apache Spark y verifica que todas las líneas se almacenen correctamente en el RDD.\n",
        "\n",
        "2. **Transformación de líneas a minúsculas y separación de palabras**: Transforma el RDD para que cada línea del archivo se convierta en minúsculas y se divida en una lista de palabras separadas por espacios. Utiliza una función lambda para realizar esta operación.\n",
        "\n",
        "3. **Definición de una función personalizada**: Implementa una función definida por el usuario que realice la misma transformación de conversión a minúsculas y separación de palabras, y aplícala al RDD original para generar un nuevo RDD transformado.\n",
        "\n",
        "4. **Almacenamiento del RDD transformado**: Guarda el RDD transformado en una carpeta denominada `salida`, utilizando la función `saveAsTextFile` de Spark.\n",
        "\n",
        "5. **Transformación con `flatMap`**: Aplica la función `flatMap` para que cada palabra de cada línea sea un elemento individual en el RDD. Muestra las primeras 10 palabras del RDD resultante utilizando la función `take()`.\n",
        "\n",
        "6. **Filtrado de palabras comunes (stopwords)**: Define una lista de stopwords que deben ser eliminadas del RDD. Aplica un filtro para eliminar todas las palabras que se encuentren en dicha lista.\n",
        "\n",
        "7. **Filtrado de palabras por la letra inicial**: Filtra el RDD para que queden solo las palabras que comiencen con la letra \"s\", utilizando la función `startswith()`.\n",
        "\n",
        "8. **Eliminación de duplicados**: Asegúrate de que el RDD no contenga palabras duplicadas utilizando la función `distinct()`.\n",
        "\n",
        "9. **Almacenamiento de los resultados finales**: Almacena el RDD filtrado y sin duplicados en una carpeta llamada `salida_final`, utilizando `saveAsTextFile` para guardar los resultados en un archivo de texto.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFas8EZtN5NX"
      },
      "outputs": [],
      "source": [
        "# Lectura del archivo\n",
        "rdd = sc.textFile(\"texto-prueba.txt\")\n",
        "rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQPSq_lxa5i4"
      },
      "outputs": [],
      "source": [
        "# Separar por palabras en cada línea\n",
        "rdd.map(lambda x: x.lower().split(' ')).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHbNQFBibHOf"
      },
      "outputs": [],
      "source": [
        "# Similar a la función lambda anterior, pero usando una función con nombre\n",
        "def funcion1(x):\n",
        "  y = x.lower()\n",
        "  z = y.split(' ')\n",
        "  return z\n",
        "\n",
        "rdd2 = rdd.map(funcion1)\n",
        "rdd2.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_woVoADEbdpG"
      },
      "outputs": [],
      "source": [
        "# Filtrar los RDDs donde aparece la palabra Python\n",
        "rdd2.filter(lambda x: \"python\" in x).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aabu_6zGbtwN"
      },
      "outputs": [],
      "source": [
        "# Almacenar un RDD (se especifica el nombre de la carpeta)\n",
        "rdd2.saveAsTextFile(\"/content/salida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrGA0Jzwb4mG"
      },
      "outputs": [],
      "source": [
        "# Dividir las palabras sin estructura (\"flat\")\n",
        "rdd3 = rdd.flatMap(lambda x: x.lower().split())\n",
        "# Mostrar solo las 10 primeras\n",
        "rdd3.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itL2jG_XchvG"
      },
      "outputs": [],
      "source": [
        "# Palabras que se eliminará\n",
        "stop_words = [\"este\", \"de\", \"es\", \"un\"]\n",
        "# Filtraje: mantener solo las palabras que no estén en \"stop_words\"\n",
        "rdd4 = rdd3.filter(lambda x: x not in stop_words)\n",
        "\n",
        "#rdd4.distinct().collect()\n",
        "rdd4.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-v9P9FbcxB-"
      },
      "outputs": [],
      "source": [
        "# Se desea solo las palabras que comienzan con s\n",
        "rdd5 = rdd3.filter(lambda x: x.startswith('s'))\n",
        "rdd5.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGMF0jlOdKqN"
      },
      "outputs": [],
      "source": [
        "# Quedarse solo con los elementos que son diferentes\n",
        "rdd5.distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIScp31ydd9G"
      },
      "source": [
        "### **Ejemplo 2: Agrupación de Palabras por Primera Letra en Apache Spark**\n",
        "\n",
        "**Objetivo**:\n",
        "\n",
        "El objetivo de este ejercicio es procesar un archivo de texto y agrupar las palabras por su primera letra utilizando Apache Spark. Esta operación es útil para organizar grandes volúmenes de datos de manera eficiente y facilitar análisis posteriores, como la búsqueda de patrones o la categorización de palabras. Al agrupar las palabras por su primera letra, estarás creando una estructura de datos organizada que permite identificar fácilmente las palabras que comienzan con cada letra del alfabeto, lo que puede ser valioso en tareas de análisis léxico o de procesamiento de lenguaje natural.\n",
        "\n",
        "**Enunciado**:\n",
        "\n",
        "1. **Carga del archivo de texto**: Carga el archivo `texto-prueba.txt` en un RDD utilizando Apache Spark y verifica que las líneas se almacenen correctamente.\n",
        "\n",
        "2. **Transformación de líneas a palabras**: Utiliza la función `flatMap` para convertir cada línea en una lista de palabras. Asegúrate de que todas las palabras se conviertan a minúsculas.\n",
        "\n",
        "3. **Conteo del número de elementos en el RDD**: Calcula el número total de palabras en el RDD utilizando la función `count()`.\n",
        "\n",
        "4. **Agrupación por primera letra**: Agrupa las palabras del RDD según la primera letra de cada palabra utilizando la función `groupBy`. Esta agrupación facilita el análisis léxico y la identificación de palabras en función de su inicial.\n",
        "\n",
        "5. **Conversión de grupos a listas**: Convierte los grupos de palabras (iterables) a listas para poder visualizar el contenido de cada grupo.\n",
        "\n",
        "Asegúrate de utilizar las funciones `flatMap`, `count`, `groupBy`, y `mapValues` de Spark para realizar las operaciones solicitadas. Los resultados deben agruparse correctamente y ser legibles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5myRR2aoPt-V"
      },
      "outputs": [],
      "source": [
        "rdd0 = sc.textFile(\"texto-prueba.txt\")\n",
        "rdd = rdd0.flatMap(lambda x: x.lower().split(' '))\n",
        "rdd.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQTD12kBQ5TP"
      },
      "outputs": [],
      "source": [
        "# Número de elementos en el RDD\n",
        "rdd.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIWBgbZfd8Tt"
      },
      "outputs": [],
      "source": [
        "# Agrupar según primera letra\n",
        "rdd2 = rdd.groupBy(lambda x: x[0])\n",
        "rdd2.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFDjMhIQePnt"
      },
      "outputs": [],
      "source": [
        "# Convertir en lista los valores que son iterables (para poder mostrarlos)\n",
        "rdd2.mapValues(list).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLMdsp0leZaN"
      },
      "outputs": [],
      "source": [
        "# Similar al anterior, pero usando herramientas nativas de Python\n",
        "[(k, list(v))  for (k, v) in rdd2.collect()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJukthyZRQr2"
      },
      "source": [
        "### **Ejemplo 3: Análisis de Calificaciones en Apache Spark**\n",
        "\n",
        "**Objetivo**:\n",
        "\n",
        "El objetivo de este ejercicio es procesar un archivo de calificaciones de usuarios, analizar las frecuencias de dichas calificaciones y visualizarlas mediante un gráfico de barras. Aprenderás a extraer información relevante de los datos, contar la frecuencia de elementos en un RDD y generar representaciones gráficas de los resultados.\n",
        "\n",
        "**Formato del archivo**:  \n",
        "El archivo `u.data` contiene cuatro columnas separadas por tabulaciones:\n",
        "- `user_id`: Identificador del usuario.\n",
        "- `item_id`: Identificador del ítem (película o producto).\n",
        "- `rating`: Calificación del ítem por parte del usuario.\n",
        "- `timestamp`: Marca de tiempo cuando se realizó la calificación.\n",
        "\n",
        "**Enunciado**:\n",
        "\n",
        "1. **Carga del archivo de calificaciones**: Carga el archivo `u.data` en un RDD utilizando Apache Spark. Visualiza las primeras tres líneas del archivo para verificar que los datos se han cargado correctamente.\n",
        "\n",
        "2. **Separación de las líneas en columnas**: Utiliza la función `map` para dividir cada línea del archivo en columnas, utilizando los espacios como delimitador.\n",
        "\n",
        "3. **Extracción de las calificaciones**: Extrae el tercer elemento de cada línea (que corresponde a la calificación) y almacénalo en un nuevo RDD llamado `ratings`.\n",
        "\n",
        "4. **Conteo de las frecuencias**: Utiliza la función `countByValue` para contar cuántas veces se repite cada calificación en el RDD.\n",
        "\n",
        "5. **Visualización de las frecuencias**: Imprime los resultados obtenidos y genera un gráfico de barras que muestre la frecuencia de cada calificación.\n",
        "\n",
        "6. **Ordenación de las calificaciones**: Ordena las calificaciones por su valor y convierte el resultado en un diccionario ordenado para su visualización final.\n",
        "\n",
        "Asegúrate de utilizar las funciones de Spark adecuadas (`map`, `countByValue`) para realizar las operaciones solicitadas. Utiliza `matplotlib` para generar el gráfico de barras y presenta los resultados de manera ordenada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uQPJ_jMeqh1"
      },
      "outputs": [],
      "source": [
        "rdd = sc.textFile(\"u.data\")\n",
        "rdd.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VJ00w3hf1On"
      },
      "outputs": [],
      "source": [
        "# Separar cada línea a partir de las tabulaciones\n",
        "rdd.map(lambda x: x.split('\\t')).take(3)\n",
        "#rdd.map(lambda x: x.split()).take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHszCRBcf7c9"
      },
      "outputs": [],
      "source": [
        "# Separar cada línea y tomar solo el elemento 3 (con índice 2) que correponde a los \"ratings\"\n",
        "ratings = rdd.map(lambda x: x.split()[2])\n",
        "ratings.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRpurVQ0gAmN"
      },
      "outputs": [],
      "source": [
        "# Contar el número de veces que se repite cada elemento del RDD (\"frecuencia\")\n",
        "resultado = ratings.countByValue()\n",
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FOSdovp6gMak"
      },
      "outputs": [],
      "source": [
        "# Mostrar los valores en un iterable\n",
        "for k, v in resultado.items():\n",
        "  print(k, v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDJsZKT2gTzM"
      },
      "outputs": [],
      "source": [
        "# Gráfico de barras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(resultado.keys(), resultado.values())\n",
        "plt.show()\n",
        "# Notar que las claves no se encuentran ordenadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_FdTk09SWWK"
      },
      "outputs": [],
      "source": [
        "# Ordenar según las claves\n",
        "ordenado = sorted(resultado.items())\n",
        "print(ordenado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_tDJoERSt-5"
      },
      "outputs": [],
      "source": [
        "#Convertir a un diccionario ordenado\n",
        "import collections\n",
        "\n",
        "resultadoOrdenado = collections.OrderedDict(ordenado)\n",
        "resultadoOrdenado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ_le0z0Suhx"
      },
      "outputs": [],
      "source": [
        "# Gráfico de barras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(resultadoOrdenado.keys(), resultadoOrdenado.values())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1r7qQEgUFQY"
      },
      "source": [
        "### Tupla con nombre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJehcyyOUDx7"
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "# Tupla con nombre llamada \"Tupla\"\n",
        "Tupla = namedtuple(\"Tupla\", [\"nombre1\", \"nombre2\"])\n",
        "\n",
        "# Creación de una tupla\n",
        "t = Tupla(40, 25)\n",
        "t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvpVI_KjU1HG"
      },
      "outputs": [],
      "source": [
        "# Acceso al primer elemento de la tupa\n",
        "print(t[0])          # Usando el índice\n",
        "print(t.nombre2)     # Usando el nombre del elemento (similar a un atributo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-rG3o-MTRrg"
      },
      "source": [
        "### **Ejemplo 4 - Análisis de Volúmenes y Fechas del Índice Dow Jones**\n",
        "\n",
        "**Objetivo**:\n",
        "\n",
        "El objetivo de este ejercicio es procesar un archivo CSV que contiene datos históricos del índice Dow Jones. Se estructurarán las filas del archivo en tuplas con nombre, se calcularán los volúmenes totales de transacciones agrupados por mes, y se identificarán los cinco meses con mayor volumen. Finalmente, los resultados serán formateados y almacenados en un archivo de salida.\n",
        "\n",
        "**Descripción del archivo**:  \n",
        "El archivo `DowJones19.csv` contiene los siguientes campos, separados por comas:\n",
        "- **Date**: Fecha de la transacción.\n",
        "- **Open**: Precio de apertura del índice.\n",
        "- **High**: Precio máximo del índice en ese día.\n",
        "- **Low**: Precio mínimo del índice en ese día.\n",
        "- **Close**: Precio de cierre del índice.\n",
        "- **Adj_Close**: Precio de cierre ajustado.\n",
        "- **Volume**: Volumen de transacciones en ese día.\n",
        "\n",
        "**Instrucciones**:\n",
        "\n",
        "1. Cargar el archivo `DowJones19.csv` en un RDD y eliminar la fila de la cabecera.\n",
        "2. Extraer los nombres de las columnas para crear una tupla con nombre (`namedtuple`) que estructurará los datos de manera accesible.\n",
        "3. Analizar las fechas presentes en el archivo, identificando la fecha mínima y la fecha máxima.\n",
        "4. Calcular la suma total de los volúmenes de transacciones en todo el conjunto de datos.\n",
        "5. Agrupar los datos por mes y calcular el volumen total de transacciones para cada mes.\n",
        "6. Identificar los cinco meses con mayor volumen de transacciones.\n",
        "7. Formatear los resultados y almacenarlos en un archivo de salida, asegurando que los datos se guarden en una única partición.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDOvFfwUTM0e"
      },
      "outputs": [],
      "source": [
        "# Lectura del archivo\n",
        "f = sc.textFile(\"DowJones19.csv\")\n",
        "\n",
        "f.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnbSaYfOVNdY"
      },
      "outputs": [],
      "source": [
        "# Recuperar los nombres de la primera fila (como una lista)\n",
        "header = f.first()\n",
        "header"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtHsbYU1ecIZ"
      },
      "outputs": [],
      "source": [
        "header_list = header.split(\",\")\n",
        "header_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8K_tLUjeb7s"
      },
      "outputs": [],
      "source": [
        "header_list[5] = \"Adj_Close\"\n",
        "header_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu3XCA2_Ve3a"
      },
      "outputs": [],
      "source": [
        "# Preprocesamiento: creación de una tupla con nombre para cada fila\n",
        "# Requiere: from collections import namedtuple\n",
        "Record = namedtuple(\"Record\", header_list)\n",
        "\n",
        "# Ejemplo de uso de tupla \"Record\"\n",
        "tupla = Record('2014-10-27', 16796.099609,16836.980469,16729.830078,16817.939453,16817.939453,72580000)\n",
        "\n",
        "print(tupla)\n",
        "print(\"Campo Volumen:\", tupla.Volume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDnD72h-e0kt"
      },
      "outputs": [],
      "source": [
        "# Eliminar la primera fila de los datos\n",
        "f2 = f.filter(lambda x: x!= header)\n",
        "f2.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4TEt8qzfEaO"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de \"parsing\": separación de cada elemento del primer componente\n",
        "elems = f2.first().split(',')\n",
        "# print(elems)\n",
        "\n",
        "# Creación de una tupla usando cada elemento por separado\n",
        "tupla = Record(elems[0], float(elems[1]), float(elems[2]), float(elems[3]),\n",
        "               float(elems[4]), float(elems[5]), float(elems[6]))\n",
        "tupla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpRxvB3qgKZA"
      },
      "outputs": [],
      "source": [
        "# Creación de una tupla usando *map\n",
        "tupla = Record(elems[0], *map(float, elems[1:]))\n",
        "tupla"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqzTd13DWcel"
      },
      "outputs": [],
      "source": [
        "# Convertir cada fila en una tupla con nombre\n",
        "def parse(x):\n",
        "  campos = x.split(\",\")\n",
        "  return Record(campos[0], *map(float, campos[1:]))\n",
        "\n",
        "# Apliar la función de conversión y almacenar en caché (persistencia de memoria)\n",
        "datos = f2.map(parse).cache()\n",
        "datos.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qLk2XABXl-3"
      },
      "outputs": [],
      "source": [
        "# Mínima fecha\n",
        "datos.map(lambda x: x.Date).min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBSlcFXGXvKG"
      },
      "outputs": [],
      "source": [
        "# Máxima fecha\n",
        "datos.map(lambda x: x.Date).max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TsmuCLIX63G"
      },
      "outputs": [],
      "source": [
        "# Suma de los volúmenes\n",
        "datos.map(lambda x: x.Volume).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn_uZmofX_Hk"
      },
      "outputs": [],
      "source": [
        "# Conversión a clave valor, donde la clave es el mes (y año)\n",
        "# datos.map(lambda x: (x.Date[0:7], x.Volume)).take(3)\n",
        "datos_mes = datos.map(lambda x: (x.Date[:7], x))\n",
        "datos_mes.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bgs2fU6YP2y"
      },
      "outputs": [],
      "source": [
        "# Volúmenes de cada mes\n",
        "\n",
        "# Reducir la tupla a (fecha, volumen)\n",
        "datos_mes_volumen = datos_mes.mapValues(lambda x: x.Volume)\n",
        "datos_mes_volumen.take(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLpdiAlqYhWg"
      },
      "outputs": [],
      "source": [
        "# Volúmen total de cada mes (reduce)\n",
        "volumen_mes = datos_mes_volumen.reduceByKey(lambda x,y: x+y)\n",
        "volumen_mes.take(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6bb1bJcYpgd"
      },
      "outputs": [],
      "source": [
        "# Volúmenes más altos por mes\n",
        "volumen_mes.top(5, lambda x: x[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9K4pGw8Y_zv"
      },
      "outputs": [],
      "source": [
        "# Asignar un formato a la salida y grabar\n",
        "salida = volumen_mes.map(lambda x: \",\".join(map(str,x)))\n",
        "salida.take(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La línea de código anterior tiene la siguiente explicación:\n",
        "\n",
        "1. **`volumen_mes.map(lambda x: ...)`**:\n",
        "   - La variable `volumen_mes` es un RDD que contiene pares clave-valor, donde la clave es el mes y el valor es el volumen total de transacciones para ese mes.\n",
        "   - `map` es una transformación de RDD que aplica una función a cada elemento del RDD. En este caso, está aplicando la función lambda (una función anónima) a cada tupla `(mes, volumen)` en `volumen_mes`.\n",
        "\n",
        "2. **`lambda x: \",\".join(map(str, x))`**:\n",
        "   - La función lambda toma un argumento `x`, que es una tupla `(mes, volumen)`.\n",
        "   - `map(str, x)` convierte cada elemento de la tupla en una cadena de texto. Es decir, si `x = ('2020-01', 15000)`, se convierte en `['2020-01', '15000']`.\n",
        "   - `\",\".join(map(str, x))` convierte la lista de cadenas en una única cadena separada por comas. En el ejemplo anterior, la salida sería: `'2020-01,15000'`.\n"
      ],
      "metadata": {
        "id": "U5rhcRhA4skc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7JRV50yZKHc"
      },
      "outputs": [],
      "source": [
        "# Almacenar (en una sola partición)\n",
        "salida.repartition(1).saveAsTextFile(\"/content/salida_ejm4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejemplo 5: Uniendo RDDs con `join`, `leftOuterJoin`, y `rightOuterJoin`\n",
        "\n",
        "Las operaciones de combinación son fundamentales en el procesamiento de datos para unir diferentes conjuntos de datos basados en claves comunes. PySpark proporciona varias funciones de combinación que permiten unir dos RDDs de formas variadas, dependiendo de los requerimientos del análisis:\n",
        "\n",
        "- **`join`**: Combina dos RDDs donde la clave existe en ambos. El resultado es un RDD de pares clave-valor, donde cada valor es un par que contiene elementos de ambos RDDs para esa clave.\n",
        "- **`leftOuterJoin`**: Combina dos RDDs pero mantiene todos los elementos del RDD de la izquierda, incluso si no hay una correspondencia en el RDD de la derecha. Los elementos del RDD derecho que no tienen una clave correspondiente en el RDD izquierdo aparecerán como `None`.\n",
        "- **`rightOuterJoin`**: Similar al `leftOuterJoin`, pero mantiene todos los elementos del RDD de la derecha, y los elementos del RDD izquierdo sin correspondencia aparecerán como `None`.\n",
        "\n",
        "Estas operaciones son esenciales para análisis donde se necesita integrar información de múltiples fuentes para obtener una vista más completa o para realizar comparaciones y análisis basados en datos agregados de varias tablas o fuentes de datos."
      ],
      "metadata": {
        "id": "Rjxdsz6RK51D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 5.1\n",
        "Consideremos un escenario donde tenemos dos conjuntos de datos: uno con información de ventas de productos y otro con detalles de los productos. Queremos combinar estos datos para obtener un informe completo que incluya el nombre del producto y sus ventas.\n",
        "\n",
        "Los datos de ventas de productos consisten en pares de (producto_id, ventas), y los detalles de los productos en pares de (producto_id, nombre_producto). Nuestro objetivo es usar la función `join` para combinar estos dos RDDs por `producto_id` y así obtener un informe que relacione cada producto con sus respectivas ventas.\n"
      ],
      "metadata": {
        "id": "Z_AG5ET3LKh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos de ventas de productos (producto_id, ventas)\n",
        "ventas_rdd = sc.parallelize([(1, 100), (2, 80), (3, 50)])\n",
        "\n",
        "# Datos de detalles de productos (producto_id, nombre_producto)\n",
        "detalles_rdd = sc.parallelize([(1, \"Smartphone\"), (2, \"Tablet\"), (3, \"Laptop\")])\n",
        "\n",
        "# Realizar un join para combinar los datos\n",
        "productos_ventas = ventas_rdd.join(detalles_rdd)\n",
        "\n",
        "# Recoger y mostrar los resultados\n",
        "resultados = productos_ventas.collect()\n",
        "print(\"Reporte de Ventas:\")\n",
        "for id, (ventas, nombre) in resultados:\n",
        "    print(f\"{nombre}: {ventas} unidades vendidas\")\n"
      ],
      "metadata": {
        "id": "-DVQWUt7LHXW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 5.2\n",
        "En este ejemplo, vamos a simular un entorno donde necesitamos cargar datos desde archivos externos. Supongamos que tenemos dos archivos: uno con ventas de productos y otro con detalles de los productos. Estos archivos están almacenados en formato CSV y queremos leer estos datos en RDDs, luego combinarlos para obtener un informe detallado que relacione las ventas con los nombres de los productos.\n",
        "\n",
        "Los archivos están estructurados de la siguiente manera:\n",
        "- **ventas_rdd.csv**: Contiene `producto_id` y `ventas`, separados por comas.\n",
        "- **detalles_rdd.csv**: Contiene `producto_id` y `nombre_producto`, separados por comas.\n",
        "\n",
        "El objetivo es leer estos archivos, crear RDDs correspondientes y usar la función `join` para combinar estos RDDs por `producto_id`. Este proceso simulará un análisis real donde se combinan datos de ventas y detalles del producto para generar informes útiles."
      ],
      "metadata": {
        "id": "Ucgzcq58MY8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGujuSrGZTrV"
      },
      "outputs": [],
      "source": [
        "# Leer los archivos en RDDs\n",
        "# Suponiendo que los archivos están en el directorio actual donde se ejecuta PySpark\n",
        "ventas_rdd = sc.textFile(\"ventas_rdd.csv\")\n",
        "detalles_rdd = sc.textFile(\"detalles_rdd.csv\")\n",
        "\n",
        "# Convertir las líneas de CSV a pares (producto_id, valor)\n",
        "ventas_rdd = ventas_rdd.map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"producto_id\").map(lambda x: (int(x[0]), int(x[1])))\n",
        "detalles_rdd = detalles_rdd.map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"producto_id\").map(lambda x: (int(x[0]), x[1]))\n",
        "\n",
        "# Realizar un join para combinar los datos basados en producto_id\n",
        "productos_ventas = ventas_rdd.join(detalles_rdd)\n",
        "\n",
        "# Recoger y mostrar los resultados\n",
        "resultados = productos_ventas.collect()\n",
        "print(\"Reporte de Ventas Combinadas:\")\n",
        "for id, (ventas, nombre) in resultados:\n",
        "    print(f\"Producto: {nombre}, Ventas: {ventas} unidades\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo 5.3\n",
        "Supongamos que estamos trabajando en un análisis para una empresa que desea entender mejor las tendencias de compra y las preferencias de sus clientes. Para ello, disponemos de tres conjuntos de datos: clientes, productos y ventas. Queremos combinar estos datos para obtener un informe detallado que relacione las ventas con los productos y los clientes.\n",
        "\n",
        "- **clientes.csv**: Contiene `cliente_id`, `nombre`, y `region`.\n",
        "- **productos.csv**: Contiene `producto_id`, `nombre_producto`, y `categoria`.\n",
        "- **ventas2.csv**: Contiene `venta_id`, `cliente_id`, `producto_id`, y `cantidad`.\n",
        "\n",
        "Nuestro objetivo es unir estos datos para crear un informe que muestre las preferencias de compra por región y categoría de producto, analizando las ventas y la distribución de los clientes.\n"
      ],
      "metadata": {
        "id": "O4jlQ5SKOr6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Suponiendo que los archivos están en el directorio actual donde se ejecuta PySpark\n",
        "# Leer los archivos en RDDs\n",
        "clientes_rdd = sc.textFile(\"clientes_rdd.csv\").map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"cliente_id\").map(lambda x: (int(x[0]), (x[1], x[2])))\n",
        "productos_rdd = sc.textFile(\"productos_rdd.csv\").map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"producto_id\").map(lambda x: (int(x[0]), (x[1], x[2])))\n",
        "ventas_rdd = sc.textFile(\"ventas2_rdd.csv\").map(lambda line: line.split(\",\")).filter(lambda line: line[0] != \"venta_id\").map(lambda x: (int(x[1]), (int(x[2]), int(x[3]))))\n",
        "\n",
        "# Unir las ventas con los productos por producto_id\n",
        "ventas_productos = ventas_rdd.map(lambda x: (x[1][0], (x[0], x[1][1]))) # re-map para producto_id como clave\n",
        "venta_detalle_producto = ventas_productos.join(productos_rdd).map(lambda x: (x[1][0][0], (x[0], x[1][0][1], x[1][1][0], x[1][1][1]))) # remap para cliente_id como clave\n",
        "\n",
        "# Unir los resultados con los clientes\n",
        "venta_detalle_final = venta_detalle_producto.join(clientes_rdd)\n",
        "\n",
        "# Recoger y mostrar los resultados\n",
        "resultados = venta_detalle_final.collect()\n",
        "print(\"Reporte Detallado de Ventas:\")\n",
        "for cliente, ((producto_id, cantidad, nombre_producto, categoria), (nombre_cliente, region)) in resultados:\n",
        "    print(f\"Cliente: {nombre_cliente}, Región: {region}, Producto: {nombre_producto}, Categoría: {categoria}, Cantidad: {cantidad}\")"
      ],
      "metadata": {
        "id": "CR8RnH77Oxzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ejercicios**"
      ],
      "metadata": {
        "id": "uFzd-_7vPBhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 1**:\n",
        "Dada una lista de precios de productos (por ejemplo, `[4.50, 2.75, 3.35, 5.80]`), crea un RDD y utiliza la función `map` para aplicar un aumento del 10% a cada precio. Recoge y muestra los resultados."
      ],
      "metadata": {
        "id": "1O1tLy-yPSvh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Q5m9iMbO3Ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 2**:\n",
        "La gerencia de una cadena de supermercados ha solicitado a un analiista de datos un informe detallado sobre las ventas mensuales de varios productos para entender cuáles merecen una promoción. El objetivo es calcular el total de ventas por producto y aplicar un descuento del 5% a aquellos productos cuyas ventas superen los $100 en el mes, como parte de una estrategia para incentivar mayores ventas.\n",
        "\n",
        "Para ello, en el último mes, se recopiló la siguiente información sobre las ventas de productos:\n",
        "\n",
        "- Manzana: 150 unidades a $0.75 cada una.\n",
        "\n",
        "- Banana: 300 unidades a $0.30 cada una.\n",
        "\n",
        "- Naranja: 200 unidades a $0.50 cada una.\n",
        "\n",
        "- Pera: 50 unidades a $1.25 cada una.\n",
        "\n",
        "- Melón: 80 unidades a $3.50 cada una.\n",
        "\n",
        "A partir de estos datos, en dónde cada registro de venta está representado como una tupla que contiene el nombre del producto, la cantidad vendida y el precio por unidad, realiza las siguientes actividades:\n",
        "\n",
        "1. Construye un RDD con los datos de ventas que has preparado.\n",
        "2. Utiliza la transformación `map` para calcular el total de ventas por producto, multiplicando la cantidad vendida por el precio por unidad.\n",
        "3. Para los productos cuyo total de ventas exceda los $100, aplica un descuento del 5%.\n",
        "4. Genera un informe final mostrando el nombre del producto, las ventas totales originales y las ventas totales después del descuento, si aplicó."
      ],
      "metadata": {
        "id": "OSh3OsYyPffc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LOy4UvSoPiLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 3**: Un equipo editorial está revisando varios artículos para identificar las palabras clave más frecuentes y relevantes, pero quieren evitar palabras comunes y cortas que no aportan significado. Se te ha proporcionado un conjunto de datos que contiene múltiples frases de artículos. Tu tarea es extraer y listar las palabras que tienen cuatro letras o más, ya que el equipo considera que estas palabras podrían ser más relevantes para sus análisis.\n",
        "\n",
        "Se han extraído varias frases de los artículos, como se muestra a continuación:\n",
        "- \"Innovación y tecnología en el mercado global\"\n",
        "- \"Explorando las profundidades del análisis de datos\"\n",
        "- \"Introducción a la programación en Python para ciencia de datos\"\n",
        "- \"Los desafíos actuales en la inteligencia artificial\"\n",
        "\n",
        "Representa estas frases en una lista de cadenas y procésalas para cumplir con el objetivo mencionado realizando las siguientes actividades:\n",
        "\n",
        "1. Crea un RDD con los datos de las frases que se han proporcionado.\n",
        "2. Utiliza la transformación `flatMap` para descomponer las frases en palabras.\n",
        "3. Filtra las palabras para retener solo aquellas con cuatro letras o más.\n",
        "4. Recopila y muestra las palabras filtradas para ver el resultado final."
      ],
      "metadata": {
        "id": "J5EAx6m8Phgu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TfqOTA4yPdKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 4**: Eres parte del equipo de operaciones de TI en una gran empresa y tu tarea es monitorear y analizar los logs del servidor para asegurarte de que todos los sistemas funcionan correctamente. Se te ha proporcionado un conjunto de entradas de log y necesitas filtrar aquellos mensajes que indican errores graves o advertencias.\n",
        "\n",
        "Las entradas de log se presentan en el siguiente formato:\n",
        "- \"2024-04-26 12:00:01 INFO Usuario ha iniciado sesión con éxito\"\n",
        "- \"2024-04-26 12:01:15 ERROR Fallo en la base de datos\"\n",
        "- \"2024-04-26 12:02:37 WARNING Memoria casi llena\"\n",
        "\n",
        "Identifica y extrae todos los logs que contienen mensajes de error (ERROR) o advertencias (WARNING).\n",
        "\n",
        "1. Crea un RDD con los datos de los logs que se han proporcionado.\n",
        "2. Utiliza la transformación `flatMap` para descomponer las entradas de log en palabras.\n",
        "3. Filtra las entradas para retener solo aquellas que contienen las palabras 'ERROR' o 'WARNING'.\n",
        "4. Recopila y muestra las entradas filtradas para ver los mensajes críticos."
      ],
      "metadata": {
        "id": "kTdxConPPr8H"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BniFyJwFPqoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 5**: Eres un analista de datos en una empresa que vende productos electrónicos en línea. La empresa está interesada en entender mejor las ventas de sus productos a lo largo del tiempo para poder ajustar las estrategias de marketing y stock. Se te ha proporcionado un conjunto de datos que contiene las ventas diarias de varios productos. Tu tarea es calcular la venta total por producto para el último mes y determinar cuál ha sido el producto más vendido.\n",
        "\n",
        "Se han registrado las ventas diarias de productos, y cada registro contiene el nombre del producto y la cantidad vendida en ese día. Ejemplos de entradas incluyen:\n",
        "- (\"Smartphone\", 5)\n",
        "- (\"Tablet\", 3)\n",
        "- (\"Laptop\", 2)\n",
        "- (\"Smartphone\", 2)\n",
        "- (\"Laptop\", 3)\n",
        "- (\"Smartwatch\", 1)\n",
        "\n",
        "**Objetivo del Ejercicio**:\n",
        "1. Crea un RDD con los datos de ventas proporcionados.\n",
        "2. Utiliza `reduceByKey` para calcular el total de ventas por producto.\n",
        "3. Encuentra el producto con la mayor cantidad de ventas en el mes.\n",
        "4. Muestra el nombre del producto más vendido y su total de ventas.\n"
      ],
      "metadata": {
        "id": "Z6WN06YEPwTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGgDjrdHPznY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 6**: Eres un analista de datos en un sitio web de comercio electrónico y te han pedido que analices el tráfico del sitio para identificar las horas pico de visitas y la distribución del tráfico por categoría de producto. El objetivo es optimizar los recursos del servidor durante las horas de mayor actividad y ajustar las campañas publicitarias según las categorías más populares.\n",
        "\n",
        "El sistema de seguimiento del sitio web genera logs que incluyen la hora exacta de la visita y la categoría del producto que el visitante exploró. Cada entrada de log tiene el siguiente formato:\n",
        "- (timestamp, categoría_producto)\n",
        "- Ejemplos de entradas de log:\n",
        "  - (\"2024-04-26 12:00:00\", \"Electrónicos\")\n",
        "  - (\"2024-04-26 12:05:00\", \"Libros\")\n",
        "  - (\"2024-04-26 12:09:00\", \"Ropa\")\n",
        "  - (\"2024-04-26 12:15:00\", \"Hogar\")\n",
        "  - (\"2024-04-26 13:00:00\", \"Electrónicos\")\n",
        "\n",
        "1. Crea un RDD con los datos de los logs proporcionados.\n",
        "2. Extrae la hora (sin minutos ni segundos) de cada timestamp y agrupa los datos por hora para determinar cuándo ocurren las horas pico.\n",
        "3. Utiliza `reduceByKey` para contar las visitas por cada categoría de producto y determinar cuáles son las más populares.\n",
        "4. Genera un reporte final que muestre las horas pico de visitas y las tres categorías más populares."
      ],
      "metadata": {
        "id": "ftQ5MPY8P0Xw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zlRGoBdUP2gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 7**: Imagina que eres un analista de datos trabajando para una agencia de marketing digital. Se te ha encargado analizar datos de redes sociales para identificar las tendencias de hashtags más populares que se utilizan en conjunto con ciertas palabras clave.\n",
        "\n",
        "Se te han proporcionado extractos de publicaciones de una red social. Estas publicaciones incluyen una mezcla de texto normal y hashtags. Por ejemplo:\n",
        "- \"Amando la vida en la playa este verano #verano #playa\"\n",
        "- \"¡Increíble tecnología que está cambiando el mundo! #innovación #tecnología\"\n",
        "- \"Todo sobre cómo iniciar un negocio en línea #emprendimiento #negocios\"\n",
        "- \"Descubre los secretos de una dieta saludable #salud #bienestar\"\n",
        "\n",
        "Tu tarea es extraer todos los hashtags y luego identificar aquellos que frecuentemente aparecen en publicaciones que contienen las palabras 'tecnología', 'salud' o 'negocios'.\n",
        "\n",
        "1. Crea un RDD con los datos de las publicaciones.\n",
        "2. Utiliza `flatMap` para descomponer las publicaciones en palabras y extraer los hashtags.\n",
        "3. Filtra las publicaciones para encontrar aquellas que contienen las palabras clave 'tecnología', 'salud' o 'negocios'.\n",
        "4. De estas publicaciones, extrae todos los hashtags y determina cuáles son los más comunes.\n",
        "5. Genera un reporte final que muestre los hashtags más comunes para cada palabra clave."
      ],
      "metadata": {
        "id": "WInw_v3CP3K0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWLv1O0UP5BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 8**: Imagina que eres un analista de datos trabajando para una agencia de marketing digital. Se te ha encargado analizar datos de redes sociales para identificar las tendencias de hashtags más populares que se utilizan en conjunto con ciertas palabras clave.\n",
        "\n",
        "Se te han proporcionado extractos de publicaciones de una red social. Estas publicaciones incluyen una mezcla de texto normal y hashtags. Por ejemplo:\n",
        "- \"Amando la vida en la playa este verano #verano #playa\"\n",
        "- \"¡Increíble tecnología que está cambiando el mundo! #innovación #tecnología\"\n",
        "- \"Todo sobre cómo iniciar un negocio en línea #emprendimiento #negocios\"\n",
        "- \"Descubre los secretos de una dieta saludable #salud #bienestar\"\n",
        "\n",
        "Tu tarea es extraer todos los hashtags y luego identificar aquellos que frecuentemente aparecen en publicaciones que contienen las palabras 'tecnología', 'salud' o 'negocios'.\n",
        "\n",
        "1. Crea un RDD con los datos de las publicaciones.\n",
        "2. Utiliza `flatMap` para descomponer las publicaciones en palabras y extraer los hashtags.\n",
        "3. Filtra las publicaciones para encontrar aquellas que contienen las palabras clave 'tecnología', 'salud' o 'negocios'.\n",
        "4. De estas publicaciones, extrae todos los hashtags y determina cuáles son los más comunes.\n",
        "5. Genera un reporte final que muestre los hashtags más comunes para cada palabra clave."
      ],
      "metadata": {
        "id": "g5pqqISFP5_v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7l8tBkpOQBZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 10**: Eres un analista de datos en una empresa que vende productos tecnológicos. Se te ha dado la tarea de crear un informe que combine información de inventario de productos con datos de ventas recientes. El objetivo es producir un listado que muestre el nombre del producto junto con las unidades vendidas en el último mes. La información se encuentra en las bases de datos:\n",
        "\n",
        "- **inventario_rdd.csv**: Contiene `producto_id` y `nombre_producto`.\n",
        "- **ventas_mensuales_rdd.csv**: Contiene `producto_id` y `unidades_vendidas`.\n",
        "\n",
        "\n",
        "1. Cargar los datos de ambos archivos en RDDs.\n",
        "2. Utilizar la función `join` para combinar los datos de inventario con los de ventas basándose en `producto_id`.\n",
        "3. Mostrar el resultado combinado que incluya el nombre del producto y las unidades vendidas."
      ],
      "metadata": {
        "id": "ImgpXMwNQD2t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqlRO-dRQDDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ejercicio 11**: Eres un analista de datos trabajando para una compañía que quiere entender mejor la eficacia de su base de datos de clientes en relación con las transacciones realizadas. Algunos registros de transacciones pueden no tener un cliente registrado debido a errores en el proceso de entrada de datos o clientes que no se registraron correctamente. La información se encuentra almacenada en las bases de datos:\n",
        "\n",
        "- **clientes2_rdd.csv**: Contiene `cliente_id`, `nombre`, y `region`.\n",
        "- **transacciones_rdd.csv**: Contiene `transaccion_id`, `cliente_id`, y `monto`.\n",
        "\n",
        "Se sabe que no todos los `cliente_id` en las transacciones tienen un correspondiente en la base de datos de clientes, y se desea identificar estas transacciones para un análisis posterior.\n",
        "\n",
        "1. Cargar los datos de ambos archivos en RDDs.\n",
        "2. Utilizar `leftOuterJoin` para unir las transacciones con los clientes.\n",
        "3. Identificar y listar todas las transacciones que no tienen un cliente registrado asociado.\n",
        "4. Mostrar el total del monto de las transacciones no registradas y el número de tales casos."
      ],
      "metadata": {
        "id": "iR9gs1gVQSYh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0A5IfNgQVDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}